{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Results\n",
    "\n",
    "## NOTE: This is still work in progress, and the description of the script is not yet finished. It will be done by the mid October 2022\n",
    "\n",
    "This jupyter notebook includes full description of the evaluation process as is now suggested. For running the evaluation in full, use [evaluate.py](https://github.com/DigilabNLCR/BibleCitations/blob/main/evaluate.py)\n",
    "\n",
    "The evaluation proces consists of several steps. Durung the process, individual CSV files are created for each step.\n",
    "1) Drop duplicate results and transform the initial 'batch_results.csv' file into CSV file with different structure (more info needed for evaluation).\n",
    "    --> creates 'UNFILTERED_batch_results.csv' file\n",
    "2) Connecting consequential results and calculting approximate match probabilities.\n",
    "    --> creates 'FILTERED_UNFILTERED_batch_results.csv' file\n",
    "3) Filtering stop-subverses.\n",
    "    --> creates 'ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n",
    "    --> + creates 'FILTERED_BY_STOP_SUBS.csv' file (so you can check these results, too)\n",
    "4) Drop 'hidden' duplicates. These are the duplicate results that have formally different query string, but actually one of the query strings contains the other.\n",
    "    --> creates 'DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n",
    "5) Marking multiple attributions. In this case the multiple attributions are not dropped, but kept with a column that suggest if the result should be dropped or not.\n",
    "    --> creates 'MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n",
    "6) Marking 'sure' citations:\n",
    "    --> creates 'FINAL_MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n",
    "\n",
    "7) Check the results by yourselves ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biblical_intertextuality_package as bip\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Defining paths. \"\"\"\n",
    "ROOT_PATH = os.getcwd()\n",
    "\n",
    "BIBLES_PATH = os.path.join(ROOT_PATH, 'Bible_files')\n",
    "DATASETS_PATH = os.path.join(ROOT_PATH, 'datasets')\n",
    "DICTS_PATH = os.path.join(ROOT_PATH, 'dictionaries')\n",
    "CORPUS_PATH = os.path.join(ROOT_PATH, 'corpuses')\n",
    "# RESULTS_PATH = os.path.join(ROOT_PATH, 'results')\n",
    "RESULTS_PATH = os.path.join(ROOT_PATH, 'PUBLIC_RESULTS')\n",
    "ALL_JSONS_PATH = os.path.join(ROOT_PATH, 'query_jsons')\n",
    "\n",
    "JOURNAL_FULLDATA_PATH = os.path.join(ROOT_PATH, 'journals_fulldata.joblib')\n",
    "\n",
    "BATCHES_FILE_PATH = os.path.join(ROOT_PATH, 'batches.csv')\n",
    "BATCH_RESULTS_FILE_PATH = os.path.join(RESULTS_PATH, 'batch_results.csv')\n",
    "\n",
    "STOP_WORDS_PATH = os.path.join(ROOT_PATH, 'stop_words.txt')\n",
    "STOP_SUBVERSES_PATH = os.path.join(ROOT_PATH, 'stop_subverses_21.txt')\n",
    "EXCLUSIVES_PATH = os.path.join(ROOT_PATH, 'exclusives.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(results_filename='batch_results.csv', delimiter=',') -> pd.core.frame.DataFrame:\n",
    "    \"\"\" This function loads selected results from the results folder. It is returned as pandas dataframe\n",
    "    \n",
    "    :param results_filename: filename of results; 'batch_results.csv' is the default parameter, as this is the default filename of results from the search functions.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=delimiter, encoding='utf-8')\n",
    "\n",
    "\n",
    "def get_verseid_queryfile(dataframe:pd.core.frame.DataFrame, row_id:int):\n",
    "    \"\"\" This function returns search properties of a given row in the results dataframe. \"\"\"\n",
    "\n",
    "    verse_id = dataframe.loc[row_id]['verse_id']\n",
    "    query_file = dataframe.loc[row_id]['query_file']\n",
    "\n",
    "    return verse_id, query_file\n",
    "\n",
    "\n",
    "def get_book_id(verse_id:str) -> str:\n",
    "    \"\"\" Gets book's ID from a verse_id (e.g. \"Gn 1:1\"). \"\"\"\n",
    "    book_id = verse_id.split(' ')[0]\n",
    "    return book_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Drop duplicate results\n",
    "+ transform the initial 'batch_results.csv' file into CSV file with different structure (more info needed for evaluation).\n",
    "\n",
    "--> creates 'UNFILTERED_batch_results.csv' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unfiltered_search_dataframe(results_filename='batch_results.csv', save=True, return_df=False):\n",
    "    \"\"\" This functions converts the preliminary results to structure same as all of the other results (filtered and improved). This is for purely statistical reasons. It only drops duplicates. \"\"\"\n",
    "    # Load results:\n",
    "    results_dataframe = load_results(results_filename)\n",
    "\n",
    "    # Load metadata from json_metadata.joblib (created with prepare_query_documents.py)\n",
    "    jsons_metadata = joblib.load(os.path.join(ROOT_PATH, 'journals_metadata.joblib'))\n",
    "\n",
    "    # Remove duplicate rows from the result dataframe\n",
    "    print('Original size of the results dataframe:', len(results_dataframe))\n",
    "    results_dataframe.drop_duplicates(subset=['verse_id', 'query_file', 'index_query_part'], keep='first', inplace=True)\n",
    "    print('Size of the results dataframe after droping duplicates:', len(results_dataframe))\n",
    "\n",
    "    # Create (empty) final results dataframe:\n",
    "    final_results = {}\n",
    "    res_id = 0\n",
    "    print_progress = 0\n",
    "    iter_ = 0\n",
    "\n",
    "    print('Dropping duplicates...')\n",
    "    for row_id in results_dataframe.index:\n",
    "        iter_ += 1\n",
    "        if print_progress == 500:\n",
    "            print('\\t', iter_, 'of', len(results_dataframe))\n",
    "            print_progress = 0\n",
    "      \n",
    "        verse_id, query_file = get_verseid_queryfile(dataframe=results_dataframe, row_id=row_id)\n",
    "\n",
    "        # NOTE: repair Syr verses to Sir (there has been a mistake in my dataset, now it is repaired but not in the initial batch_results.csv file in PUBLIC_RESULTS)\n",
    "        if 'Syr' in verse_id:\n",
    "            verse_id = verse_id.replace('Syr', 'Sir')\n",
    "\n",
    "        row_dict = results_dataframe.loc[row_id].to_dict()\n",
    "\n",
    "        # NOTE: 334149b0-877c-11e6-8aeb-5ef3fc9ae867 has wrong date --> it is repaired here in the process:\n",
    "        if '334149b0-877c-11e6-8aeb-5ef3fc9ae867' in row_dict['query_file']:\n",
    "            row_dict['date'] = '30.06.1935'\n",
    "        else:\n",
    "            row_dict['date'] = jsons_metadata[query_file]['issue_date']\n",
    "\n",
    "        row_dict['verse_id'] = verse_id\n",
    "        row_dict['book'] = get_book_id(verse_id)\n",
    "        row_dict['journal'] = jsons_metadata[query_file]['journal']\n",
    "        row_dict['page_num'] = jsons_metadata[query_file]['issue_page']\n",
    "\n",
    "        # NOTE: filtering out year out of the scope of 1925-1939 (because for some reason, some other years also entered out dataset, possibly due to wrong metadata)\n",
    "        issue_year = row_dict['date'].split('.')[-1]\n",
    "        years_to_consider = ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '1937-1938']\n",
    "        if issue_year not in years_to_consider:\n",
    "            continue\n",
    "        else:\n",
    "            final_results[res_id] = row_dict\n",
    "            res_id += 1\n",
    "        \n",
    "        print_progress += 1\n",
    "\n",
    "    final_results_df = pd.DataFrame.from_dict(final_results)\n",
    "    final_results_df = final_results_df.transpose()\n",
    "    \n",
    "    if save:\n",
    "        final_results_df.to_csv(os.path.join(RESULTS_PATH, f'UNFILTERED_{results_filename}'), encoding='utf-8', quotechar='\"', sep=';')\n",
    "\n",
    "    if return_df:\n",
    "        return final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_unfiltered_search_dataframe(results_filename='batch_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Connecting consequential results\n",
    "- In this part the citations that appear over the borders of the split query parts are connected, so there are no duplicates of this kind.\n",
    "+ approximate match probabilities are \"calculated\"\n",
    "- This probability calculation works more or less as a suggestion for you and the suggested values are not something that can be taken really seriously.\n",
    "    - in general, the match probability is matched_characters*matched_subverses_score, i.e. it takes into consideration how large part of the verse is matched (and how big the edit distance is)\n",
    "- In addition, the scripts check if there are some \"exclusives\" irregularities? E.g. \"He said a sentence\" vs. \"I said a sentence\" (while the edit distance is in tolerance, the sentences should probably not be considered as \"the same\" ... The value is either True or False and may help you with the by-hand evaluation ... These exclusive words can be defined in [exclusives.txt](https://github.com/DigilabNLCR/BibleCitations/blob/main/exclusives.txt) - always put mutualy exclusives words on one line.\n",
    "\n",
    "--> creates 'FILTERED_UNFILTERED_batch_results.csv' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define mutually exclusive words in exclusives.txt \"\"\"\n",
    "with open(EXCLUSIVES_PATH, 'r', encoding='utf-8') as exclusives_file:\n",
    "    data = exclusives_file.read()\n",
    "    words_lines = data.split('\\n')\n",
    "\n",
    "    exclusives_dict = defaultdict(list)\n",
    "    list_of_exclusives = []\n",
    "\n",
    "    for line in words_lines:\n",
    "        word_list = line.split(', ')\n",
    "        for word_from in word_list:\n",
    "            for word_to in word_list:\n",
    "                if word_from == word_to:\n",
    "                    continue\n",
    "                if word_from == 'je' and word_to == 'jest':\n",
    "                    continue\n",
    "                if word_from == 'jest' and word_to == 'je':\n",
    "                    continue\n",
    "                else:\n",
    "                    exclusives_dict[bip.normalize_string(word_from)].append(bip.normalize_string(word_to))\n",
    "            list_of_exclusives.append(bip.normalize_string(word_from))\n",
    "\n",
    "\n",
    "def exclusiveness_test(subverse_string:str, query_string:str) -> bool:\n",
    "    \"\"\"\n",
    "    This function serves to check if the detected string is not false positive based on mutually exclusive words. E.g. naše vs. vaše;, je vs, není etc.\n",
    "    \"\"\"\n",
    "    subverse_string = bip.normalize_string(subverse_string)\n",
    "    query_string = bip.normalize_string(query_string)\n",
    "\n",
    "    subverse_words = bip.word_tokenize_no_punctuation(subverse_string)\n",
    "    query_words = bip.word_tokenize_no_punctuation(query_string)\n",
    "\n",
    "    for i, word in enumerate(subverse_words):\n",
    "        if word in list_of_exclusives:\n",
    "            list_to_ex = exclusives_dict[word]\n",
    "            try:\n",
    "                if query_words[i] in list_to_ex:\n",
    "                    return False\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_row_data_for_initial_filter(dataframe:pd.core.frame.DataFrame, row_id:int):\n",
    "    \"\"\" This function returns search properties of a given row in the results dataframe to be used in check_results function. \"\"\"\n",
    "\n",
    "    query_file = dataframe.loc[row_id]['query_file']\n",
    "    query_window_len = dataframe.loc[row_id]['query_window_len']\n",
    "    query_overlap = dataframe.loc[row_id]['query_overlap']\n",
    "\n",
    "    return query_file, query_window_len, query_overlap\n",
    "\n",
    "\n",
    "def get_verse_et_idx(dataframe:pd.core.frame.DataFrame, row_id:int):\n",
    "    \"\"\" This function returns search properties of a given row in the results dataframe. \"\"\"\n",
    "\n",
    "    verse_id = dataframe.loc[row_id, 'verse_id']\n",
    "    index_query_part = dataframe.loc[row_id, 'index_query_part']\n",
    "\n",
    "    return verse_id, index_query_part\n",
    "\n",
    "\n",
    "def select_attributions_to_json(dataframe:pd.core.frame.DataFrame, query_file:str):\n",
    "    \"\"\" This function selects all attributions to a given JSON file. \n",
    "    \n",
    "    It returns: dataframe of all of the results, row_ids to skip\n",
    "    \"\"\"\n",
    "    subset_dataframe = dataframe[dataframe['query_file'] == query_file]\n",
    "\n",
    "    # If the subset dataframe contains only one result, return it and empty skips.\n",
    "    if len(subset_dataframe) == 1:\n",
    "        verse_id, index_query_part = get_verse_et_idx(subset_dataframe, subset_dataframe.index[0])\n",
    "        attributed_verses = {verse_id: [index_query_part]}\n",
    "        return attributed_verses, []\n",
    "\n",
    "    # If the subset dataframe contains more rows, check if further.\n",
    "    else:\n",
    "        row_ids_to_skip = subset_dataframe.index\n",
    "        attributed_verses = defaultdict(list)\n",
    "        for row_id in row_ids_to_skip:\n",
    "            verse_id, index_query_part = get_verse_et_idx(dataframe=subset_dataframe, row_id=row_id)\n",
    "            attributed_verses[verse_id].append(index_query_part)\n",
    "\n",
    "        return attributed_verses, row_ids_to_skip\n",
    "\n",
    "\n",
    "def join_overlap(list_of_parts:list, query_index:int) -> str:\n",
    "    \"\"\" This function serves to join two parts of a query into one string (when the citation has been discovered in two consecutive parts of the query document). \"\"\"\n",
    "    output = ''\n",
    "\n",
    "    sentences_in_1 = sent_tokenize(list_of_parts[query_index])\n",
    "    try:\n",
    "        sentences_in_2 = sent_tokenize(list_of_parts[query_index+1])\n",
    "    except IndexError:\n",
    "        print(sentences_in_1)\n",
    "        print(list_of_parts[-1])\n",
    "\n",
    "    for sent_1 in sentences_in_1:\n",
    "        if sent_1 not in sentences_in_2:\n",
    "            output += sent_1 + ' '\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for sent_2 in sentences_in_2:\n",
    "        output += sent_2 + ' '\n",
    "\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "def fuzzy_string_matching_for_implementation_with_text(subverse_string:str, query_string:str, tolerance=0.85):\n",
    "    \"\"\" \n",
    "    Contrary to fuzzy_string_matching_for_implementation(), this function also returns the matched part of the query string and the edit distance of the compared strings. The function is duplicated so as not to speed down the function in the broad search. However, the speed difference has not been tested yet.\n",
    "\n",
    "    This function is for implementation of typo similarity detection applied to two strings. It returns bool value of match.\n",
    "\n",
    "    :param subverse_string: string of the biblical subverse we are searching for.\n",
    "    :param query_string: string in which we are searching for the seubverse_string.\n",
    "    :param tolerance: how large proportion of the subverse_string must be present in query_string to consider it a match.\n",
    "    \"\"\"\n",
    "    subverse_string = bip.normalize_string(subverse_string)\n",
    "    subverse_len = len(subverse_string)\n",
    "\n",
    "    query_string = bip.normalize_string(query_string)\n",
    "    query_len = len(query_string)\n",
    "\n",
    "    tolerance = subverse_len * (1-tolerance)\n",
    "\n",
    "    if subverse_len-tolerance > query_len:\n",
    "        # If subverse is longer than query string, it is not a match by default\n",
    "        return False, '', 0\n",
    "    elif subverse_len-tolerance <= query_len <= subverse_len+tolerance:\n",
    "        # If subverse is more or les of the same length as query string, just compare them.\n",
    "        edit_distance = Levenshtein.distance(subverse_string, query_string)\n",
    "        if edit_distance <= tolerance:\n",
    "            return True, query_string, edit_distance\n",
    "    else:\n",
    "        char_len_sub = len(subverse_string)\n",
    "        word_len_subv = len(word_tokenize(subverse_string))\n",
    "        words_in_query_string = word_tokenize(query_string)\n",
    "        word_len_query_string = len(words_in_query_string)\n",
    "\n",
    "        for i, cycle in enumerate(range(word_len_subv, word_len_query_string+1)):\n",
    "            gram_str = ' '.join(words_in_query_string[i:])[:char_len_sub]\n",
    "            edit_distance = Levenshtein.distance(subverse_string, gram_str)\n",
    "            if edit_distance <= tolerance:\n",
    "                return True, gram_str, edit_distance\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    return False, '', 0\n",
    "\n",
    "\n",
    "def check_for_verse(verse_id:str, string_to_check:str) -> dict:\n",
    "    \"\"\" This function performs the inner check for a verse in all availiable translations. It is implemented in the check_results() function. \"\"\"\n",
    "    possible_citations = []\n",
    "\n",
    "    for trsl in bip.all_translations:\n",
    "        verse_text = bip.get_verse_text(trsl, verse_id, print_exceptions=False)\n",
    "        \n",
    "        if verse_text:\n",
    "            subverses = bip.split_verse(verse_text, tole_len=21, return_shorts=True, short_limit=9)\n",
    "\n",
    "            fuzzy_matched_subs_num = 0\n",
    "            fuzzy_matched_subs = []\n",
    "            matched_subs_edit_distance = 0\n",
    "            matched_subs_chars = 0\n",
    "            exclusive_matched_subs_num = 0\n",
    "\n",
    "            for subverse in subverses:\n",
    "                # check for every subverse in edit distance\n",
    "                fuzzy_match, query_match, edit_distance = fuzzy_string_matching_for_implementation_with_text(subverse, query_string=string_to_check, tolerance=0.85)\n",
    "                if fuzzy_match:\n",
    "                    fuzzy_matched_subs_num += 1\n",
    "                    fuzzy_matched_subs.append(subverse)\n",
    "                    matched_subs_edit_distance += edit_distance\n",
    "                    matched_subs_chars += len(subverse)\n",
    "\n",
    "                    # run the exclussiveness test\n",
    "                    if exclusiveness_test(subverse, query_match):\n",
    "                        exclusive_matched_subs_num += 1\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if fuzzy_matched_subs_num == 0:\n",
    "                continue\n",
    "            else:\n",
    "                matched_characters = (matched_subs_chars-matched_subs_edit_distance)/matched_subs_chars\n",
    "                matched_subverses_score = fuzzy_matched_subs_num/len(subverses)\n",
    "\n",
    "                match_probability = matched_characters*matched_subverses_score\n",
    "\n",
    "                result_for_trsl = {'verse_id': verse_id,\n",
    "                                    'verse_text': verse_text, \n",
    "                                    'matched_subverses': fuzzy_matched_subs, \n",
    "                                    'query_string': string_to_check, \n",
    "                                    'matched_characters': (matched_subs_chars-matched_subs_edit_distance)/matched_subs_chars, \n",
    "                                    'matched_subverses_score': fuzzy_matched_subs_num/len(subverses),\n",
    "                                    'exclusives_match': exclusive_matched_subs_num/fuzzy_matched_subs_num,\n",
    "                                    'match_probability': match_probability}\n",
    "            \n",
    "                possible_citations.append(result_for_trsl)\n",
    "\n",
    "    # If there are none possible citation (which is weird and it should not happen and it probably means that there are differently split verses in the originally used BibleDataset) return result that are basically False:\n",
    "    if not possible_citations:\n",
    "        result = {'verse_id': verse_id,\n",
    "                    'verse_text': verse_text, \n",
    "                    'matched_subverses': [], \n",
    "                    'query_string': string_to_check, \n",
    "                    'matched_characters': 0, \n",
    "                    'matched_subverses_score': 0,\n",
    "                    'exclusives_match': 0,\n",
    "                    'match_probability': 'FALSE'}\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Now, if the results seem OK, select the best match (translations as such are not evaluated, just select the best result of all possible results)... in this evaluation, we consider the result with most detected subverses as a match, if same then based on the characters, and finally on the exclusiveness test results.\n",
    "    matched_subverses_scores = [pc['matched_subverses'] for pc in possible_citations]\n",
    "    matched_characters_scores = [pc['matched_characters'] for pc in possible_citations]\n",
    "    exclusiveness_test_scores = [pc['exclusives_match'] for pc in possible_citations]\n",
    "\n",
    "    # Check subverses score results:\n",
    "    best_subverses_match = max(matched_subverses_scores)\n",
    "    if matched_subverses_scores.count(best_subverses_match) == 1:\n",
    "        best_pc_idx = matched_subverses_scores.index(best_subverses_match)\n",
    "        return possible_citations[best_pc_idx]\n",
    "    else:\n",
    "        # check the character scores results:\n",
    "        idxs = [i for i, score in enumerate(matched_subverses_scores) if score == best_subverses_match]\n",
    "        best_chars_match = max([matched_characters_scores[i] for i in idxs])\n",
    "        if matched_characters_scores.count(best_chars_match) == 1:\n",
    "            best_pc_idx = matched_characters_scores.index(best_chars_match)\n",
    "            return possible_citations[best_pc_idx]\n",
    "        else:\n",
    "            # check exclusiveness test results:\n",
    "            idxs = [i for i, score in enumerate(matched_characters_scores) if score == best_chars_match]\n",
    "            best_excl_res = max([exclusiveness_test_scores[i] for i in idxs])\n",
    "            best_pc_idx = exclusiveness_test_scores.index(best_excl_res)\n",
    "            return possible_citations[best_pc_idx]\n",
    "\n",
    "\n",
    "def load_data_from_journals_fulldata(journals_fulldata:dict, query_file:str):\n",
    "    journal = journals_fulldata[query_file]['journal']\n",
    "    issue_date = journals_fulldata[query_file]['issue_date']\n",
    "    issue_page = journals_fulldata[query_file]['issue_page']\n",
    "    issue_uuid = journals_fulldata[query_file]['issue_uuid']\n",
    "    kramerius_url = journals_fulldata[query_file]['kramerius_url']\n",
    "    full_query_string = journals_fulldata[query_file]['text']\n",
    "\n",
    "    return journal, issue_date, issue_page, issue_uuid, kramerius_url, full_query_string\n",
    "\n",
    "\n",
    "def evaluate_attributions_in_doc(attributed_verses:dict, query_file:str, query_window_len:int, query_overlap:int, journals_fulldata:dict) -> list:\n",
    "    \"\"\" This function evaluates attributed verses, supposedly detected in a single JSON file. \"\"\"\n",
    "    # Load data from journals_fulldata dictionary:\n",
    "    journal, issue_date, issue_page, issue_uuid, kramerius_url, full_query_string = load_data_from_journals_fulldata(journals_fulldata=journals_fulldata, query_file=query_file)\n",
    "    \n",
    "    # NOTE: repair wrong date with 334149b0-877c-11e6-8aeb-5ef3fc9ae867\n",
    "    if '334149b0-877c-11e6-8aeb-5ef3fc9ae867' in issue_uuid:\n",
    "        issue_date = '30.06.1935'\n",
    "\n",
    "    query_parts = bip.split_query(full_query_string, window_len=query_window_len, overlap=query_overlap)\n",
    "    \n",
    "    results_of_attributions = []\n",
    "  \n",
    "    for verse_id in attributed_verses:\n",
    "        attributed_idxs = attributed_verses[verse_id]\n",
    "        if len(attributed_idxs) == 1:\n",
    "            string_to_check = query_parts[attributed_idxs[0]]\n",
    "            possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "            results_of_attributions.append(possible_citation)\n",
    "            \n",
    "        else:\n",
    "            skip = False\n",
    "            for i, q_idx in enumerate(attributed_idxs):\n",
    "                if not skip:\n",
    "                    try:\n",
    "                        if attributed_idxs[i+1] == q_idx+1:\n",
    "                            # checking if the next part is a joined sequence\n",
    "                            skip = True\n",
    "                            string_to_check = join_overlap(query_parts, q_idx)\n",
    "                            possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "                            results_of_attributions.append(possible_citation)\n",
    "                        else:\n",
    "                            string_to_check = query_parts[attributed_idxs[0]]\n",
    "                            possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "                            results_of_attributions.append(possible_citation)\n",
    "                    except IndexError:\n",
    "                        string_to_check = query_parts[attributed_idxs[i]]\n",
    "                        possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "                        results_of_attributions.append(possible_citation)\n",
    "                else:\n",
    "                    skip = False\n",
    "                    continue\n",
    "\n",
    "    # TODO: zde pak přidat všechny další parametry nalezené citace --> pak se to vrátí a přidá do výsledného DF.\n",
    "    if len(results_of_attributions) == 1:\n",
    "        results_of_attributions[0]['multiple_attribution'] = False\n",
    "        results_of_attributions[0]['journal'] = journal\n",
    "        results_of_attributions[0]['date'] = issue_date\n",
    "        results_of_attributions[0]['page_num'] = issue_page\n",
    "        results_of_attributions[0]['uuid'] = issue_uuid\n",
    "        results_of_attributions[0]['kramerius_url'] = kramerius_url\n",
    "    else:\n",
    "        for res in results_of_attributions:\n",
    "            res['multiple_attribution'] = True\n",
    "            res['journal'] = journal\n",
    "            res['date'] = issue_date\n",
    "            res['page_num'] = issue_page\n",
    "            res['uuid'] = issue_uuid\n",
    "            res['kramerius_url'] = kramerius_url\n",
    "\n",
    "    return results_of_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filtered_search_dataframe(results_filename='UNFILTERED_batch_results.csv', save=True, return_df=False):\n",
    "    \"\"\" This functions applies initial checks on the preliminary results. \"\"\"\n",
    "    # Load results:\n",
    "    print('Loading UNFILTERED results...')\n",
    "    results_dataframe = load_results(results_filename, delimiter=';')\n",
    "\n",
    "    # Load journals_fulldata:\n",
    "    print('Loading journals_fulldata.joblib...')\n",
    "    journals_full_data = joblib.load(os.path.join(ROOT_PATH, 'journals_fulldata.joblib'))\n",
    "\n",
    "    # Create (empty) final results dataframe:\n",
    "    final_results = {}\n",
    "    res_id = 0\n",
    "\n",
    "    print('Running initial filtering...')\n",
    "    rows_to_skip = []\n",
    "    print_progress = 0\n",
    "    iter_ = 0\n",
    "    for row_id in results_dataframe.index:\n",
    "        print_progress += 1\n",
    "        iter_ += 1\n",
    "        if print_progress >= 500:\n",
    "            print('\\t', iter_, '/', len(results_dataframe))\n",
    "            print_progress = 0\n",
    "\n",
    "        if row_id in rows_to_skip:\n",
    "            continue\n",
    "        else:\n",
    "            query_file, query_window_len, query_overlap = get_row_data_for_initial_filter(dataframe=results_dataframe, row_id=row_id)\n",
    "            attributed_verses, add_to_skip = select_attributions_to_json(dataframe=results_dataframe, query_file=query_file)\n",
    "            rows_to_skip.extend(add_to_skip)\n",
    "\n",
    "            results = evaluate_attributions_in_doc(attributed_verses=attributed_verses, query_file=query_file, query_window_len=query_window_len, query_overlap=query_overlap, journals_fulldata=journals_full_data)\n",
    "\n",
    "            for res in results:\n",
    "                final_results[res_id] = res\n",
    "                res_id += 1\n",
    "\n",
    "    final_results_df = pd.DataFrame.from_dict(final_results)\n",
    "    final_results_df = final_results_df.transpose()\n",
    "    \n",
    "    if save:\n",
    "        final_results_df.to_csv(os.path.join(RESULTS_PATH, f'FILTERED_{results_filename}'), encoding='utf-8', quotechar='\"', sep=';')\n",
    "\n",
    "    if return_df:\n",
    "        return final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_filtered_search_dataframe(results_filename='UNFILTERED_batch_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Filtering stop-subverses.\n",
    "- define evaluation stop subverses in [evaluation_stop_subverse_21.txt](https://github.com/DigilabNLCR/BibleCitations/blob/main/evaluation_stop_subverses_21.txt) ... the structure of this file must reflect the structure of matched_subverses column of the result dataset. I.e., ['subverse text']\n",
    "    - During this step, the citations that include only the subverses defined in [evaluation_stop_subverse_21.txt](https://github.com/DigilabNLCR/BibleCitations/blob/main/evaluation_stop_subverses_21.txt) are ignored.\n",
    "    - You should choose the subverses to filter wisely, some of the subverses that seem not worty may indeed be true bible quotes.\n",
    "- In addition, there is [100_hit_needed_subs_21.txt](https://github.com/DigilabNLCR/BibleCitations/blob/main/100_hit_needed_subs_21.txt) file that includes those verses that need to be cited in full (the string of subverse must exactly match the passage where it is supposed to be cited, i.e. no \"typos\" allowed)\n",
    "\n",
    "--> creates 'ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n",
    "--> + creates 'FILTERED_BY_STOP_SUBS.csv' file (so you can check the filtered out citations, too)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_subs(results_filename='FILTERED_UNFILTERED_batch_results.csv', input_df=False, subverse_len=21, rewrite_original_csv=False, save=True, return_df=False, save_filtered_out_file=True, filtered_out_filename='FILTERED_BY_STOP_SUBS.csv'):\n",
    "    \"\"\"\n",
    "    This function filters those results that are detected based on solely one subverse that is listed in file evaluation_stop_subverses_{subverse_len}.txt\n",
    "    \n",
    "    Also, there are subverses in evaluation_stop_subverses_{subverse_len}.txt' that need 100% hit in characters in order to be taken seriously...\n",
    "    \"\"\"\n",
    "    if input_df is not False:\n",
    "        original_df = input_df\n",
    "    else:\n",
    "        print('Loading results dataframe ...')\n",
    "        original_df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=';', encoding='utf-8', index_col=0)\n",
    "\n",
    "    print('Length of the original dataframe is:', len(original_df))\n",
    "\n",
    "    print(f'Loading stop-subverses from evaluation_stop_subverses_{subverse_len}.txt ...')\n",
    "    with open(os.path.join(ROOT_PATH, f'evaluation_stop_subverses_{subverse_len}.txt'), 'r', encoding='utf-8') as stops_f:\n",
    "        data = stops_f.read()\n",
    "        stop_subs = data.split('\\n')\n",
    "\n",
    "    print(f'Loading subverses that need 100 % hit from 100_hit_needed_subs_{subverse_len}.txt ...')\n",
    "    with open(os.path.join(ROOT_PATH, f'evaluation_stop_subverses_{subverse_len}.txt'), 'r', encoding='utf-8') as stops_f:\n",
    "        data = stops_f.read()\n",
    "        full_hit_subs = data.split('\\n')\n",
    "\n",
    "    print('Number of stop subverses to filter:', len(set(stop_subs)))\n",
    "\n",
    "    filtered_df_dict = {}\n",
    "    filtered_out_dict = {}\n",
    "    fil_id = 0\n",
    "    fil_out_id = 0  \n",
    "\n",
    "    print('Filtering rows ...')\n",
    "    for row_id in original_df.index:\n",
    "        # If the hit sontains only the stop-subverse, filter it\n",
    "        if original_df.loc[row_id]['matched_subverses'] in stop_subs:\n",
    "            row_as_dict = original_df.loc[row_id].to_dict()\n",
    "            filtered_out_dict[fil_out_id] = row_as_dict\n",
    "            fil_out_id += 1\n",
    "        \n",
    "        # If the hit contains only the subverse that need 100% hit, check it\n",
    "        elif original_df.loc[row_id]['matched_subverses'] in full_hit_subs:\n",
    "            matched_chars = eval(original_df.loc[row_id]['matched_characters'])\n",
    "            if matched_chars == 1:\n",
    "                row_as_dict = original_df.loc[row_id].to_dict()            \n",
    "                filtered_df_dict[fil_id] = row_as_dict\n",
    "                fil_id += 1\n",
    "            else:\n",
    "                row_as_dict = original_df.loc[row_id].to_dict()\n",
    "                filtered_out_dict[fil_out_id] = row_as_dict\n",
    "                fil_out_id += 1\n",
    "\n",
    "        # Otherwise, take the citation as OK\n",
    "        else:\n",
    "            row_as_dict = original_df.loc[row_id].to_dict()            \n",
    "            filtered_df_dict[fil_id] = row_as_dict\n",
    "            fil_id += 1\n",
    "\n",
    "    filtered_df = pd.DataFrame.from_dict(filtered_df_dict)\n",
    "    filtered_df = filtered_df.transpose()\n",
    "\n",
    "    print('Length of the filtered dataframe is:', len(filtered_df))\n",
    "    print('Number of filtered rows:', len(original_df)-len(filtered_df))\n",
    "\n",
    "    if rewrite_original_csv:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if save:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, f'ST_SUBS_{results_filename}'), quotechar='\"', sep=';', encoding='utf-8')\n",
    "\n",
    "    if save_filtered_out_file:\n",
    "        filtered_out_df = pd.DataFrame.from_dict(filtered_out_dict)\n",
    "        filtered_out_df = filtered_out_df.transpose()\n",
    "\n",
    "        filtered_out_df.to_csv(os.path.join(RESULTS_PATH, filtered_out_filename), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if return_df:\n",
    "        return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_stop_subs(results_filename='FILTERED_UNFILTERED_batch_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Drop 'hidden' duplicates\n",
    "These are the duplicate results that have formally different query string, but actually one of the query strings contains the other (this means that there was an overlap in the split of query document, most likely in the end of the query documents)\n",
    "\n",
    "--> creates 'DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_data_for_overlap_dups(dataframe:pd.core.frame.DataFrame, row_id:int):\n",
    "    \"\"\" This function returns search properties of a given row in the results dataframe to be used in check_results function. \"\"\"\n",
    "\n",
    "    verse_id = dataframe.loc[row_id]['verse_id']\n",
    "    uuid = dataframe.loc[row_id]['uuid']\n",
    "    page_num = dataframe.loc[row_id]['page_num']\n",
    "\n",
    "    return verse_id, uuid, page_num\n",
    "\n",
    "\n",
    "def select_attributions_to_same_page_et_verse(dataframe:pd.core.frame.DataFrame, uuid:str, page_num:int, verse_id:str):\n",
    "    \"\"\" This function selects all attributions to a given uuid, page nuber and verse ID\n",
    "    \n",
    "    It returns: dataframe of all of the results, row_ids to skip\n",
    "    \"\"\"\n",
    "    subset_dataframe = dataframe[dataframe['uuid'] == uuid]\n",
    "    subset_dataframe = subset_dataframe[subset_dataframe['page_num'] == page_num]\n",
    "    subset_dataframe = subset_dataframe[subset_dataframe['verse_id'] == verse_id]\n",
    "\n",
    "    return subset_dataframe, subset_dataframe.index\n",
    "\n",
    "\n",
    "def is_string_in_other_string(str_0:str, str_1:str):\n",
    "    \"\"\" This function checks if one of two string contain the other. It returns bool and what string to discard \"\"\"\n",
    "    if str_0 in str_1:\n",
    "        return True, 0\n",
    "    elif str_1 in str_0:\n",
    "        return True, 1\n",
    "    else:\n",
    "        return False, None\n",
    "\n",
    "\n",
    "def compare_overlapped_query_string(subset_dataframe:pd.core.frame.DataFrame):\n",
    "    \"\"\" This function compares query strings of a subset dataframe (as filtered by select_attributions_to_same_page_et_verse function). \"\"\"\n",
    "    query_strings = {}\n",
    "    for row_id in subset_dataframe.index:\n",
    "        query_string = subset_dataframe.loc[row_id]['query_string']\n",
    "        query_strings[row_id] = query_string\n",
    "\n",
    "    output_rows = []\n",
    "    rows_dropped = 0\n",
    "    for qs_a in query_strings:\n",
    "        for qs_b in query_strings:\n",
    "            if qs_a == qs_b:\n",
    "                continue\n",
    "            else:\n",
    "                is_overlap, qs_to_drop = is_string_in_other_string(query_strings[qs_a], query_strings[qs_b])\n",
    "                if is_overlap:\n",
    "                    if qs_to_drop == 1:\n",
    "                        row_dict = subset_dataframe.loc[qs_a].to_dict()\n",
    "                        output_rows.append(row_dict)\n",
    "                        rows_dropped += 1\n",
    "                else:\n",
    "                    row_dict = subset_dataframe.loc[qs_a].to_dict()\n",
    "                    output_rows.append(row_dict)\n",
    "\n",
    "    return output_rows, rows_dropped\n",
    "\n",
    "\n",
    "def filter_duplicates_by_overlap(results_filename='ST_SUBS_FILTERED_UNFILTERED_batch_results.csv', input_df=False, subverse_len=21, rewrite_original_csv=False, save=True, return_df=False):\n",
    "    \"\"\" This function filters those duplicates that do not seem as full duplicates, because the query string is different. However, sometimes (due to overlaps), there are some matches that include full query string of other match. \"\"\"\n",
    "    if input_df is not False:\n",
    "        results_dataframe = input_df\n",
    "    else:\n",
    "        print('Loading results dataframe ...')\n",
    "        results_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=';', encoding='utf-8', index_col=0)\n",
    "\n",
    "    print('Length of the original dataframe is:', len(results_dataframe))\n",
    "\n",
    "    # Create (empty) final results dataframe:\n",
    "    final_results = {}\n",
    "    res_id = 0\n",
    "\n",
    "    print('Filtering \"hidden\" duplicates...')\n",
    "    rows_to_skip = []\n",
    "    rows_dropped = 0\n",
    "    res_id = 0\n",
    "    for row_id in results_dataframe.index:\n",
    "        if row_id in rows_to_skip:\n",
    "            continue\n",
    "        else:\n",
    "            verse_id, uuid, page_num = get_row_data_for_overlap_dups(dataframe=results_dataframe, row_id=row_id)\n",
    "            \n",
    "            subset_dataframe, add_to_skip = select_attributions_to_same_page_et_verse(dataframe=results_dataframe, uuid=uuid, page_num=page_num, verse_id=verse_id)\n",
    "            rows_to_skip.extend(add_to_skip)\n",
    "\n",
    "            if len(subset_dataframe) == 1:\n",
    "                row_dict = results_dataframe.loc[row_id].to_dict()\n",
    "                final_results[res_id] = row_dict\n",
    "                res_id += 1\n",
    "            else:\n",
    "                row_dicts, num_of_rows_dropped = compare_overlapped_query_string(subset_dataframe=subset_dataframe)\n",
    "                rows_dropped += num_of_rows_dropped\n",
    "                for rd in row_dicts:\n",
    "                    final_results[res_id] = rd\n",
    "                    res_id += 1\n",
    "\n",
    "    final_results_df = pd.DataFrame.from_dict(final_results)\n",
    "    final_results_df = final_results_df.transpose()\n",
    "\n",
    "    print('Number of dropped rows:', rows_dropped)\n",
    "\n",
    "    if rewrite_original_csv:\n",
    "        final_results_df.to_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if save:\n",
    "        final_results_df.to_csv(os.path.join(RESULTS_PATH, f'DUPS_{results_filename}'), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if return_df:\n",
    "        return final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_duplicates_by_overlap(results_filename='ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Marking multiple attributions\n",
    "In this case the multiple attributions are not dropped, but kept with a column that suggest if the result should be dropped or not.\n",
    "--> creates 'MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_multiply_attributed_rows(dataframe:pd.core.frame.DataFrame, row_id):\n",
    "    \"\"\" This finction selects all rows that share same multiple attribution. \"\"\"\n",
    "    uuid = dataframe.loc[row_id]['uuid']\n",
    "    query_string = dataframe.loc[row_id]['query_string']\n",
    "\n",
    "    other_attributions_df = dataframe[dataframe['uuid'] == uuid]\n",
    "    other_attributions_df = other_attributions_df[other_attributions_df['multiple_attribution'] == True]\n",
    "    other_attributions_df = other_attributions_df[other_attributions_df['query_string'] == query_string]\n",
    "\n",
    "    row_ids_to_skip = other_attributions_df.index\n",
    "\n",
    "    return other_attributions_df, row_ids_to_skip\n",
    "\n",
    "\n",
    "def fuzzy_string_matching_for_multiple_attributions(subverse_string:str, query_string:str, tolerance=0.85):\n",
    "    \"\"\" \n",
    "    This function is used to evaluate multiple attributions within the same query string. The function returns parts of query string thatare the supposed match - if these overlap then it is multiple attribution, if not, there are probably just more verses cited within one passage.\n",
    "\n",
    "    :param subverse_string: string of the biblical subverse we are searching for.\n",
    "    :param query_string: string in which we are searching for the seubverse_string.\n",
    "    :param tolerance: how large proportion of the subverse_string must be present in query_string to consider it a match.\n",
    "    \"\"\"\n",
    "    subverse_string = bip.normalize_string(subverse_string)\n",
    "    subverse_len = len(subverse_string)\n",
    "\n",
    "    query_string = bip.normalize_string(query_string)\n",
    "    query_len = len(query_string)\n",
    "\n",
    "    tolerance = subverse_len * (1-tolerance)\n",
    "\n",
    "    if subverse_len-tolerance > query_len:\n",
    "        # If subverse is longer than query string, it is not a match by default\n",
    "        return ()\n",
    "    elif subverse_len-tolerance <= query_len <= subverse_len+tolerance:\n",
    "        # If subverse is more or les of the same length as query string, just compare them.\n",
    "        edit_distance = Levenshtein.distance(subverse_string, query_string)\n",
    "        if edit_distance <= tolerance:\n",
    "            return (0, len(query_string))\n",
    "    else:\n",
    "        char_len_sub = len(subverse_string)\n",
    "        word_len_subv = len(word_tokenize(subverse_string))\n",
    "        words_in_query_string = word_tokenize(query_string)\n",
    "        word_len_query_string = len(words_in_query_string)\n",
    "\n",
    "        for i, cycle in enumerate(range(word_len_subv, word_len_query_string+1)):\n",
    "            gram_str = ' '.join(words_in_query_string[i:])[:char_len_sub]\n",
    "            edit_distance = Levenshtein.distance(subverse_string, gram_str)\n",
    "            if edit_distance <= tolerance:\n",
    "                return (i, (char_len_sub+i))\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    return ()\n",
    "\n",
    "\n",
    "def check_consecutive(input_list:list):\n",
    "    return sorted(input_list) == list(range(min(input_list), max(input_list)+1))\n",
    "\n",
    "\n",
    "def return_ranges_if_not_consecutives(full_range:list):\n",
    "    \"\"\" This function returns ranges of non-consecutive match. \"\"\"\n",
    "    ranges = []\n",
    "    range_start = full_range[0]\n",
    "    for i, value in enumerate(full_range):\n",
    "        try:\n",
    "            if value+1 == full_range[i+1]:\n",
    "                continue\n",
    "            else:\n",
    "                ranges.append((range_start, value))\n",
    "                range_start = full_range[i+1]\n",
    "        except IndexError:\n",
    "            ranges.append((range_start, value))\n",
    "            continue\n",
    "    \n",
    "    return ranges\n",
    "\n",
    "\n",
    "def make_full_range(subverses:list, query_string:str):\n",
    "    \"\"\" This function returns the full range of matched subverses in the query string and bool whether the matched subverses are consecutively present in the query string. \"\"\"\n",
    "    subs_range = []\n",
    "    for sub in subverses:\n",
    "        gram_str_range = fuzzy_string_matching_for_multiple_attributions(sub, query_string)\n",
    "        for i in range(gram_str_range[0], gram_str_range[1]+1):\n",
    "            if i not in subs_range:\n",
    "                subs_range.append(i)\n",
    "    \n",
    "    if check_consecutive(subs_range):\n",
    "        return [(min(subs_range), max(subs_range))]\n",
    "    \n",
    "    else:\n",
    "        subs_range.sort()\n",
    "        return return_ranges_if_not_consecutives(subs_range)\n",
    "\n",
    "\n",
    "def make_all_ranges_into_all_other_ranges(all_ranges_list:list, list_to_remove:list):\n",
    "    \"\"\" This function removes matched ranges of a selected subverse from the list of all ranges, so it is compared only with other ranges, not with itself. Theoretically this should not be really a problem, but I find it better to deal with it anyhow.  \"\"\"\n",
    "    reduced_list = all_ranges_list.copy()\n",
    "    for item_to_remove in list_to_remove:\n",
    "        reduced_list.remove(item_to_remove)\n",
    "\n",
    "    return reduced_list\n",
    "\n",
    "\n",
    "def check_overlap_of_subs_ranges(subs_ranges:list):\n",
    "    \"\"\"\n",
    "    This function checks if there are overlaps between ranges of subverses within the query string.\n",
    "    \n",
    "    :param subs_ranges: list of lists of ranges (e.g., [[(1,3)], [(5,6), (8,15)]])\n",
    "    \"\"\"\n",
    "    overlap_stats = []\n",
    "    for a, s_range_a in enumerate(subs_ranges):\n",
    "        overlaps_of_sub_a = {}\n",
    "        for b, s_range_b in enumerate(subs_ranges):\n",
    "            if a == b:\n",
    "                continue\n",
    "            else:\n",
    "                current_stat = False\n",
    "                for a_range in s_range_a:\n",
    "                    for b_range in s_range_b:\n",
    "                        try:\n",
    "                            for i in range(a_range[0], a_range[1]+1):\n",
    "                                if i in range(b_range[0], b_range[1]+1):\n",
    "                                    current_stat = True\n",
    "                                    break\n",
    "\n",
    "                        except IndexError:\n",
    "                            continue\n",
    "                overlaps_of_sub_a[b] = current_stat\n",
    "\n",
    "        overlap_stats.append(overlaps_of_sub_a)\n",
    "    \n",
    "    return overlap_stats\n",
    "\n",
    "\n",
    "def evaluate_multiple_attributions(subset_dataframe:pd.core.frame.DataFrame):\n",
    "    \"\"\" This function evaluates the DROP value of respective rows. \"\"\"\n",
    "    match_probability_values = []\n",
    "    verse_ids = []\n",
    "    matched_subverses = []\n",
    "    query_strings = []\n",
    "    for row_id in subset_dataframe.index:\n",
    "        match_probability_values.append(subset_dataframe.loc[row_id]['match_probability'])\n",
    "        matched_subverses.append(eval(subset_dataframe.loc[row_id]['matched_subverses']))\n",
    "        verse_ids.append(subset_dataframe.loc[row_id]['verse_id'])\n",
    "        query_strings.append(subset_dataframe.loc[row_id]['query_string'])\n",
    "    \n",
    "    matched_ranges = []\n",
    "    for matched_subs in matched_subverses:\n",
    "        # NOTE: the query_strings should be all the same, so we can just select the first one\n",
    "        sub_ranges = make_full_range(matched_subs, query_string=query_strings[0])\n",
    "        matched_ranges.append(sub_ranges)\n",
    "    \n",
    "    # Get overlap states of each mach to all other matches.\n",
    "    overlap_stats = check_overlap_of_subs_ranges(matched_ranges)\n",
    "\n",
    "    output_dicts = []\n",
    "\n",
    "    num_of_rows_to_drop = 0\n",
    "    \n",
    "    for i, mpv in enumerate(match_probability_values):\n",
    "        # get overlap states of the matchon position \"i\":\n",
    "        i_overlaps = overlap_stats[i]\n",
    "        overlapped_positions = [i]\n",
    "        for i_match in i_overlaps:\n",
    "            # If there is some other match that overlaps with this specific match, get its id\n",
    "            if i_overlaps[i_match]:\n",
    "                overlapped_positions.append(i_match)\n",
    "\n",
    "        # If there are some overlaps, we will drop the current match if it does not have the highest probability value of all overlapping matches\n",
    "        if len(overlapped_positions) > 1:\n",
    "            scores_of_overlapped_matches = []\n",
    "            for op in overlapped_positions:\n",
    "                scores_of_overlapped_matches.append(match_probability_values[op])\n",
    "            if mpv == max(scores_of_overlapped_matches):\n",
    "                to_drop = False\n",
    "            else:\n",
    "                to_drop = True\n",
    "                num_of_rows_to_drop += 1\n",
    "\n",
    "        # But if there are no overlaps, we will not drop this match:\n",
    "        else:\n",
    "            to_drop = False\n",
    "\n",
    "        df_dict = subset_dataframe.loc[subset_dataframe.index[i]].to_dict()\n",
    "        df_dict['drop?'] = to_drop\n",
    "        output_dicts.append(df_dict)\n",
    "\n",
    "    return output_dicts, num_of_rows_to_drop\n",
    "\n",
    "\n",
    "def mark_multiple_attributions(results_filename='DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv', input_df=False, rewrite_original_csv=False, save=True, return_df=False):\n",
    "    \"\"\" This function suggest which of the multiple attribution is the right one. \"\"\"\n",
    "    if input_df is not False:\n",
    "        original_df = input_df\n",
    "    else:\n",
    "        print('Loading results dataframe ...')\n",
    "        original_df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=';', encoding='utf-8', index_col=0)\n",
    "\n",
    "    output_df_dict = {}\n",
    "    out_idx = 0\n",
    "\n",
    "    num_of_rows_to_drop = 0\n",
    "\n",
    "    rows_to_skip = []\n",
    "\n",
    "    print('Evaluating multiple attributions ...')\n",
    "    for row_id in original_df.index:\n",
    "        if row_id in rows_to_skip:\n",
    "            continue\n",
    "        else:\n",
    "            if original_df.loc[row_id]['multiple_attribution']:\n",
    "                other_attributions_df, add_to_skip = select_multiply_attributed_rows(dataframe=original_df, row_id=row_id)\n",
    "                rows_to_skip.extend(add_to_skip)\n",
    "                if len(other_attributions_df) == 1:\n",
    "                    row_as_dict = original_df.loc[row_id].to_dict()\n",
    "                    row_as_dict['drop?'] = False\n",
    "                    output_df_dict[out_idx] = row_as_dict\n",
    "                    out_idx += 1\n",
    "                else:\n",
    "                    rows_to_add, rows_to_drop_count = evaluate_multiple_attributions(subset_dataframe=other_attributions_df)\n",
    "                    num_of_rows_to_drop += rows_to_drop_count\n",
    "                    for rta in rows_to_add:\n",
    "                        output_df_dict[out_idx] = rta\n",
    "                        out_idx += 1\n",
    "            else:\n",
    "                row_as_dict = original_df.loc[row_id].to_dict()\n",
    "                row_as_dict['drop?'] = False\n",
    "                output_df_dict[out_idx] = row_as_dict\n",
    "                out_idx += 1\n",
    "\n",
    "    filtered_df = pd.DataFrame.from_dict(output_df_dict)\n",
    "    filtered_df = filtered_df.transpose()\n",
    "\n",
    "    print('Number of rows selected for drop:', num_of_rows_to_drop, 'out of', len(original_df))\n",
    "\n",
    "    if rewrite_original_csv:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if save:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, f'MA_{results_filename}'), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if return_df:\n",
    "        return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_multiple_attributions(results_filename='DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Marking 'sure' citations\n",
    "- We consider a 'sure' citation one that is cited either in full (all subverses and without any typos) or almost in full in the case of larger nuber of subverses. In addition, it reflects the \"exclusive\" value (some words are marked as mustually exclusive - e.g., \"je\" and \"není\" = \"is\" and \"is not\")\n",
    "- --> creates 'FINAL_MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_sure_citations(results_filename='MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv', input_df=False, rewrite_original_csv=False, save=True, return_df=False):\n",
    "    \"\"\" This function marks some of the citations as sure citations while other unsure. \"\"\"\n",
    "    if input_df is not False:\n",
    "        original_df = input_df\n",
    "    else:\n",
    "        print('Loading results dataframe ...')\n",
    "        original_df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=';', encoding='utf-8', index_col=0)\n",
    "\n",
    "    output_df_dict = {}\n",
    "    out_idx = 0\n",
    "\n",
    "    num_of_sure_citations = 0\n",
    "\n",
    "    print('Marking \"sure\" citations ...')\n",
    "    for row_id in original_df.index:\n",
    "        row_as_dict = original_df.loc[row_id].to_dict()\n",
    "\n",
    "        num_of_subverses_in_verse = len(bip.split_verse(row_as_dict['verse_text'], tole_len=21))\n",
    "        match_subs_score = row_as_dict['matched_subverses_score']\n",
    "\n",
    "        if num_of_subverses_in_verse <= 2:\n",
    "            if match_subs_score == 1 and row_as_dict['exclusives_match'] == 1:\n",
    "                row_as_dict['CITATION'] = True\n",
    "                num_of_sure_citations += 1\n",
    "            else:\n",
    "                row_as_dict['CITATION'] = False\n",
    "        elif num_of_subverses_in_verse <= 4:\n",
    "            if match_subs_score >= 0.8 and row_as_dict['exclusives_match'] == 1:\n",
    "                row_as_dict['CITATION'] = True\n",
    "                num_of_sure_citations += 1\n",
    "            else:\n",
    "                row_as_dict['CITATION'] = False\n",
    "\n",
    "        else:\n",
    "            if match_subs_score >= 0.5 and row_as_dict['exclusives_match'] == 1:\n",
    "                row_as_dict['CITATION'] = True\n",
    "                num_of_sure_citations += 1\n",
    "            else:\n",
    "                row_as_dict['CITATION'] = False\n",
    "\n",
    "        output_df_dict[out_idx] = row_as_dict\n",
    "        out_idx += 1\n",
    "    \n",
    "    filtered_df = pd.DataFrame.from_dict(output_df_dict)\n",
    "    filtered_df = filtered_df.transpose()\n",
    "\n",
    "    print('Number of rows selected as sure citations:', num_of_sure_citations, 'out of', len(original_df))\n",
    "\n",
    "    if rewrite_original_csv:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if save:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, f'SURE_{results_filename}'), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if return_df:\n",
    "        return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_sure_citations(results_filename='MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Working out the \"same\" verses\n",
    "- Some of the verses are actually \"the same\" in their content and thus are detected more times. These verses are connected into one citation refferencing all of the citations by the following process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define mutual verses (those that are \"the same\"). \"\"\"\n",
    "\n",
    "mutual_verses_layout = {\n",
    "    'L 11:3/Mt 6:11': ['L 11:3', 'Mt 6:11'],\n",
    "    'Mk 13:31/Mt 24:35/L 21:33': ['Mk 13:31', 'Mt 24:35', 'L 21:33'],\n",
    "    'Ex 20:16/Dt 5:20': ['Ex 20:16', 'Dt 5:20'],\n",
    "    'Ex 20:15/Dt 5:19': ['Ex 20:15', 'Dt 5:19'],\n",
    "    'Ex 20:14/Dt 5:18': ['Ex 20:14', 'Dt 5:18'],\n",
    "    'Ex 20:13/Dt 5:17': ['Ex 20:13', 'Dt 5:17'],\n",
    "    'Ex 20:12/Dt 5:16': ['Ex 20:12', 'Dt 5:16'],\n",
    "    'Ex 20:7/Dt 5:11': ['Ex 20:7', 'Dt 5:11'],\n",
    "    'Ex 20:2/Dt 5:6': ['Ex 20:2', 'Dt 5:6'],\n",
    "    'Ex 20:3/Dt 5:7': ['Ex 20:3', 'Dt 5:7'],\n",
    "    '2K 1:2/Fp 1:2/2Te 1:2/1K 1:3/Ef 1:2/Ga 1:3': ['2K 1:2', 'Fp 1:2', '2Te 1:2', '1K 1:3', 'Ef 1:2', 'Ga 1:3'],\n",
    "    'Mt 11:15/Mt 13:9': ['Mt 11:15', 'Mt 13:9'],\n",
    "    'Mt 6:11/L 11:3': ['Mt 6:11', 'L 11:3']\n",
    "}\n",
    "\n",
    "mutual_verses = {}\n",
    "for mv in mutual_verses_layout:\n",
    "    for verse_id in mutual_verses_layout[mv]:\n",
    "        mutual_verses[verse_id] = mv\n",
    "\n",
    "\n",
    "def filter_mutual_verses(results_filename='FINAL_MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv', delimiter=';'):\n",
    "    \"\"\" This filters those verses that \"are the same\" (e.g. \"Nezabiješ\" is both Ex 20:16 and Dt 5:20). \"\"\"\n",
    "    # Load results:\n",
    "    print('Loading results...')\n",
    "    results_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=delimiter, encoding='utf-8', index_col=0)\n",
    "\n",
    "    # Create (empty) final results dataframe:\n",
    "    final_results = {}\n",
    "    res_id = 0\n",
    "\n",
    "    print('Running filtering...')\n",
    "    rows_to_skip = []\n",
    "\n",
    "    for row_id in results_dataframe.index:\n",
    "\n",
    "        row_as_dict = results_dataframe.loc[row_id].to_dict()\n",
    "        verse_id = row_as_dict['verse_id']\n",
    "\n",
    "        if verse_id in mutual_verses:\n",
    "            row_as_dict['verse_id'] = mutual_verses[verse_id]\n",
    "            if f'{row_as_dict[\"verse_id\"]}{row_as_dict[\"query_string\"]}' in rows_to_skip:\n",
    "                continue\n",
    "            else:\n",
    "                rows_to_skip.append(f'{row_as_dict[\"verse_id\"]}{row_as_dict[\"query_string\"]}')\n",
    "                final_results[res_id] = row_as_dict\n",
    "                res_id += 1\n",
    "\n",
    "        else:\n",
    "            final_results[res_id] = row_as_dict\n",
    "            res_id += 1\n",
    "\n",
    "    final_results_df = pd.DataFrame.from_dict(final_results)\n",
    "    final_results_df = final_results_df.transpose()\n",
    "    \n",
    "    final_results_df.to_csv(os.path.join(RESULTS_PATH, f'MUTUAL_DROP_{results_filename}'), encoding='utf-8', quotechar='\"', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results...\n",
      "Running filtering...\n"
     ]
    }
   ],
   "source": [
    "filter_mutual_verses()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8427d73f02270cccde5d7b657eb9efbd549974ced0a163dd64bdafb1c87004cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
