{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Results\n",
    "\n",
    "## NOTE: This is still work in progress, and the description of the script is not yet finished. It will be done by the mid October 2022\n",
    "\n",
    "This jupyter notebook includes full description of the evaluation process as is now suggested. For running the evaluation in full, use [evaluate.py](https://github.com/DigilabNLCR/BibleCitations/blob/main/evaluate.py)\n",
    "\n",
    "The evaluation proces consists of several steps. Durung the process, individual CSV files are created for each step.\n",
    "1) Drop duplicate results and transform the initial 'batch_results.csv' file into CSV file with different structure (more info needed for evaluation).\n",
    "    --> creates 'UNFILTERED_batch_results.csv' file\n",
    "2) Connecting consequential results and calculting approximate match probabilities.\n",
    "    --> creates 'FILTERED_UNFILTERED_batch_results.csv' file\n",
    "3) Filtering stop-subverses.\n",
    "    --> creates 'ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n",
    "    --> + creates 'FILTERED_BY_STOP_SUBS.csv' file (so you can check these results, too)\n",
    "4) Drop 'hidden' duplicates. These are the duplicate results that have formally different query string, but actually one of the query strings contains the other.\n",
    "    --> creates 'DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n",
    "5) Marking multiple attributions. In this case the multiple attributions are not dropped, but kept with a column that suggest if the result should be dropped or not.\n",
    "    --> creates 'MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n",
    "6) Marking 'sure' citations:\n",
    "    --> creates 'FINAL_MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n",
    "\n",
    "7) Check the results by yourselves ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biblical_intertextuality_package as bip\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Defining paths. \"\"\"\n",
    "ROOT_PATH = os.getcwd()\n",
    "\n",
    "BIBLES_PATH = os.path.join(ROOT_PATH, 'Bible_files')\n",
    "DATASETS_PATH = os.path.join(ROOT_PATH, 'datasets')\n",
    "DICTS_PATH = os.path.join(ROOT_PATH, 'dictionaries')\n",
    "CORPUS_PATH = os.path.join(ROOT_PATH, 'corpuses')\n",
    "RESULTS_PATH = os.path.join(ROOT_PATH, 'results')\n",
    "ALL_JSONS_PATH = os.path.join(ROOT_PATH, 'query_jsons')\n",
    "\n",
    "JOURNAL_FULLDATA_PATH = os.path.join(ROOT_PATH, 'journals_fulldata.joblib')\n",
    "\n",
    "BATCHES_FILE_PATH = os.path.join(ROOT_PATH, 'batches.csv')\n",
    "BATCH_RESULTS_FILE_PATH = os.path.join(RESULTS_PATH, 'batch_results.csv')\n",
    "\n",
    "STOP_WORDS_PATH = os.path.join(ROOT_PATH, 'stop_words.txt')\n",
    "STOP_SUBVERSES_PATH = os.path.join(ROOT_PATH, 'stop_subverses_21.txt')\n",
    "EXCLUSIVES_PATH = os.path.join(ROOT_PATH, 'exclusives.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(results_filename='batch_results.csv', delimiter=',') -> pd.core.frame.DataFrame:\n",
    "    \"\"\" This function loads selected results from the results folder. It is returned as pandas dataframe\n",
    "    \n",
    "    :param results_filename: filename of results; 'batch_results.csv' is the default parameter, as this is the default filename of results from the search functions.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=delimiter, encoding='utf-8')\n",
    "\n",
    "\n",
    "def get_verseid_queryfile(dataframe:pd.core.frame.DataFrame, row_id:int):\n",
    "    \"\"\" This function returns search properties of a given row in the results dataframe. \"\"\"\n",
    "\n",
    "    verse_id = dataframe.loc[row_id]['verse_id']\n",
    "    query_file = dataframe.loc[row_id]['query_file']\n",
    "\n",
    "    return verse_id, query_file\n",
    "\n",
    "\n",
    "def get_book_id(verse_id:str) -> str:\n",
    "    \"\"\" Gets book's ID from a verse_id (e.g. \"Gn 1:1\"). \"\"\"\n",
    "    book_id = verse_id.split(' ')[0]\n",
    "    return book_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Drop duplicate results\n",
    "+ transform the initial 'batch_results.csv' file into CSV file with different structure (more info needed for evaluation).\n",
    "--> creates 'UNFILTERED_batch_results.csv' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unfiltered_search_dataframe(results_filename='batch_results.csv', save=True, return_df=False):\n",
    "    \"\"\" This functions converts the preliminary results to structure same as all of the other results (filtered and improved). This is for purely statistical reasons. It only drops duplicates. \"\"\"\n",
    "    # Load results:\n",
    "    results_dataframe = load_results(results_filename)\n",
    "\n",
    "    # Load metadata from json_metadata.joblib (created with prepare_query_documents.py)\n",
    "    jsons_metadata = joblib.load(os.path.join(ROOT_PATH, 'journals_metadata.joblib'))\n",
    "\n",
    "    # Remove duplicate rows from the result dataframe\n",
    "    print('Original size of the results dataframe:', len(results_dataframe))\n",
    "    results_dataframe.drop_duplicates(subset=['verse_id', 'query_file', 'index_query_part'], keep='first', inplace=True)\n",
    "    print('Size of the results dataframe after droping duplicates:', len(results_dataframe))\n",
    "\n",
    "    # Create (empty) final results dataframe:\n",
    "    final_results = {}\n",
    "    res_id = 0\n",
    "    print_progress = 0\n",
    "    iter_ = 0\n",
    "\n",
    "    print('Dropping duplicates...')\n",
    "    for row_id in results_dataframe.index:\n",
    "        iter_ += 1\n",
    "        if print_progress == 500:\n",
    "            print('\\t', iter_, 'of', len(results_dataframe))\n",
    "            print_progress = 0\n",
    "      \n",
    "        verse_id, query_file = get_verseid_queryfile(dataframe=results_dataframe, row_id=row_id)\n",
    "\n",
    "        # NOTE: repair Syr verses to Sir (there has been a mistake in my dataset, now it is repaired but not in the initial batch_results.csv file in PUBLIC_RESULTS)\n",
    "        if 'Syr' in verse_id:\n",
    "            verse_id = verse_id.replace('Syr', 'Sir')\n",
    "\n",
    "        row_dict = results_dataframe.loc[row_id].to_dict()\n",
    "\n",
    "        # NOTE: 334149b0-877c-11e6-8aeb-5ef3fc9ae867 has wrong date --> it is repaired here in the process:\n",
    "        if '334149b0-877c-11e6-8aeb-5ef3fc9ae867' in row_dict['query_file']:\n",
    "            row_dict['date'] = '30.06.1935'\n",
    "        else:\n",
    "            row_dict['date'] = jsons_metadata[query_file]['issue_date']\n",
    "\n",
    "        row_dict['verse_id'] = verse_id\n",
    "        row_dict['book'] = get_book_id(verse_id)\n",
    "        row_dict['journal'] = jsons_metadata[query_file]['journal']\n",
    "        row_dict['page_num'] = jsons_metadata[query_file]['issue_page']\n",
    "\n",
    "        # NOTE: filtering out year out of the scope of 1925-1939 (because for some reason, some other years also entered out dataset, possibly due to wrong metadata)\n",
    "        issue_year = row_dict['date'].split('.')[-1]\n",
    "        years_to_consider = ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '1937-1938']\n",
    "        if issue_year not in years_to_consider:\n",
    "            continue\n",
    "        else:\n",
    "            final_results[res_id] = row_dict\n",
    "            res_id += 1\n",
    "        \n",
    "        print_progress += 1\n",
    "\n",
    "    final_results_df = pd.DataFrame.from_dict(final_results)\n",
    "    final_results_df = final_results_df.transpose()\n",
    "    \n",
    "    if save:\n",
    "        final_results_df.to_csv(os.path.join(RESULTS_PATH, f'UNFILTERED_{results_filename}'), encoding='utf-8', quotechar='\"', sep=';')\n",
    "\n",
    "    if return_df:\n",
    "        return final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_unfiltered_search_dataframe(results_filename='batch_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Connecting consequential results\n",
    "- In this part the citations that appear over the borders of the split query parts are connected, so there are no duplicates of this kind.\n",
    "+ approximate match probabilities are \"calculated\"\n",
    "- This probability calculation works more or less as a suggestion for you and the suggested values are not something that can be taken really seriously.\n",
    "    - in general, the match probability is matched_characters*matched_subverses_score, i.e. it takes into consideration how large part of the verse is matched (and how big the edit distance is)\n",
    "- In addition, the scripts check if there are some \"exclusives\" irregularities? E.g. \"He said a sentence\" vs. \"I said a sentence\" (while the edit distance is in tolerance, the sentences should probably not be considered as \"the same\" ... The value is either True or False and may help you with the by-hand evaluation ... These exclusive words can be defined in [exclusives.txt](https://github.com/DigilabNLCR/BibleCitations/blob/main/exclusives.txt) - always put mutualy exclusives words on one line.\n",
    "\n",
    "--> creates 'FILTERED_UNFILTERED_batch_results.csv' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define mutually exclusive words in exclusives.txt \"\"\"\n",
    "with open(EXCLUSIVES_PATH, 'r', encoding='utf-8') as exclusives_file:\n",
    "    data = exclusives_file.read()\n",
    "    words_lines = data.split('\\n')\n",
    "\n",
    "    exclusives_dict = defaultdict(list)\n",
    "    list_of_exclusives = []\n",
    "\n",
    "    for line in words_lines:\n",
    "        word_list = line.split(', ')\n",
    "        for word_from in word_list:\n",
    "            for word_to in word_list:\n",
    "                if word_from == word_to:\n",
    "                    continue\n",
    "                if word_from == 'je' and word_to == 'jest':\n",
    "                    continue\n",
    "                if word_from == 'jest' and word_to == 'je':\n",
    "                    continue\n",
    "                else:\n",
    "                    exclusives_dict[bip.normalize_string(word_from)].append(bip.normalize_string(word_to))\n",
    "            list_of_exclusives.append(bip.normalize_string(word_from))\n",
    "\n",
    "\n",
    "def exclusiveness_test(subverse_string:str, query_string:str) -> bool:\n",
    "    \"\"\"\n",
    "    This function serves to check if the detected string is not false positive based on mutually exclusive words. E.g. naše vs. vaše;, je vs, není etc.\n",
    "    \"\"\"\n",
    "    subverse_string = bip.normalize_string(subverse_string)\n",
    "    query_string = bip.normalize_string(query_string)\n",
    "\n",
    "    subverse_words = bip.word_tokenize_no_punctuation(subverse_string)\n",
    "    query_words = bip.word_tokenize_no_punctuation(query_string)\n",
    "\n",
    "    for i, word in enumerate(subverse_words):\n",
    "        if word in list_of_exclusives:\n",
    "            list_to_ex = exclusives_dict[word]\n",
    "            try:\n",
    "                if query_words[i] in list_to_ex:\n",
    "                    return False\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_row_data_for_initial_filter(dataframe:pd.core.frame.DataFrame, row_id:int):\n",
    "    \"\"\" This function returns search properties of a given row in the results dataframe to be used in check_results function. \"\"\"\n",
    "\n",
    "    query_file = dataframe.loc[row_id]['query_file']\n",
    "    query_window_len = dataframe.loc[row_id]['query_window_len']\n",
    "    query_overlap = dataframe.loc[row_id]['query_overlap']\n",
    "\n",
    "    return query_file, query_window_len, query_overlap\n",
    "\n",
    "\n",
    "def get_verse_et_idx(dataframe:pd.core.frame.DataFrame, row_id:int):\n",
    "    \"\"\" This function returns search properties of a given row in the results dataframe. \"\"\"\n",
    "\n",
    "    verse_id = dataframe.loc[row_id, 'verse_id']\n",
    "    index_query_part = dataframe.loc[row_id, 'index_query_part']\n",
    "\n",
    "    return verse_id, index_query_part\n",
    "\n",
    "\n",
    "def select_attributions_to_json(dataframe:pd.core.frame.DataFrame, query_file:str):\n",
    "    \"\"\" This function selects all attributions to a given JSON file. \n",
    "    \n",
    "    It returns: dataframe of all of the results, row_ids to skip\n",
    "    \"\"\"\n",
    "    subset_dataframe = dataframe[dataframe['query_file'] == query_file]\n",
    "\n",
    "    # If the subset dataframe contains only one result, return it and empty skips.\n",
    "    if len(subset_dataframe) == 1:\n",
    "        verse_id, index_query_part = get_verse_et_idx(subset_dataframe, subset_dataframe.index[0])\n",
    "        attributed_verses = {verse_id: [index_query_part]}\n",
    "        return attributed_verses, []\n",
    "\n",
    "    # If the subset dataframe contains more rows, check if further.\n",
    "    else:\n",
    "        row_ids_to_skip = subset_dataframe.index\n",
    "        attributed_verses = defaultdict(list)\n",
    "        for row_id in row_ids_to_skip:\n",
    "            verse_id, index_query_part = get_verse_et_idx(dataframe=subset_dataframe, row_id=row_id)\n",
    "            attributed_verses[verse_id].append(index_query_part)\n",
    "\n",
    "        return attributed_verses, row_ids_to_skip\n",
    "\n",
    "\n",
    "def join_overlap(list_of_parts:list, query_index:int) -> str:\n",
    "    \"\"\" This function serves to join two parts of a query into one string (when the citation has been discovered in two consecutive parts of the query document). \"\"\"\n",
    "    output = ''\n",
    "\n",
    "    sentences_in_1 = sent_tokenize(list_of_parts[query_index])\n",
    "    try:\n",
    "        sentences_in_2 = sent_tokenize(list_of_parts[query_index+1])\n",
    "    except IndexError:\n",
    "        print(sentences_in_1)\n",
    "        print(list_of_parts[-1])\n",
    "\n",
    "    for sent_1 in sentences_in_1:\n",
    "        if sent_1 not in sentences_in_2:\n",
    "            output += sent_1 + ' '\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for sent_2 in sentences_in_2:\n",
    "        output += sent_2 + ' '\n",
    "\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "def fuzzy_string_matching_for_implementation_with_text(subverse_string:str, query_string:str, tolerance=0.85):\n",
    "    \"\"\" \n",
    "    Contrary to fuzzy_string_matching_for_implementation(), this function also returns the matched part of the query string and the edit distance of the compared strings. The function is duplicated so as not to speed down the function in the broad search. However, the speed difference has not been tested yet.\n",
    "\n",
    "    This function is for implementation of typo similarity detection applied to two strings. It returns bool value of match.\n",
    "\n",
    "    :param subverse_string: string of the biblical subverse we are searching for.\n",
    "    :param query_string: string in which we are searching for the seubverse_string.\n",
    "    :param tolerance: how large proportion of the subverse_string must be present in query_string to consider it a match.\n",
    "    \"\"\"\n",
    "    subverse_string = bip.normalize_string(subverse_string)\n",
    "    subverse_len = len(subverse_string)\n",
    "\n",
    "    query_string = bip.normalize_string(query_string)\n",
    "    query_len = len(query_string)\n",
    "\n",
    "    tolerance = subverse_len * (1-tolerance)\n",
    "\n",
    "    if subverse_len-tolerance > query_len:\n",
    "        # If subverse is longer than query string, it is not a match by default\n",
    "        return False, '', 0\n",
    "    elif subverse_len-tolerance <= query_len <= subverse_len+tolerance:\n",
    "        # If subverse is more or les of the same length as query string, just compare them.\n",
    "        edit_distance = Levenshtein.distance(subverse_string, query_string)\n",
    "        if edit_distance <= tolerance:\n",
    "            return True, query_string, edit_distance\n",
    "    else:\n",
    "        char_len_sub = len(subverse_string)\n",
    "        word_len_subv = len(word_tokenize(subverse_string))\n",
    "        words_in_query_string = word_tokenize(query_string)\n",
    "        word_len_query_string = len(words_in_query_string)\n",
    "\n",
    "        for i, cycle in enumerate(range(word_len_subv, word_len_query_string+1)):\n",
    "            gram_str = ' '.join(words_in_query_string[i:])[:char_len_sub]\n",
    "            edit_distance = Levenshtein.distance(subverse_string, gram_str)\n",
    "            if edit_distance <= tolerance:\n",
    "                return True, gram_str, edit_distance\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    return False, '', 0\n",
    "\n",
    "\n",
    "def check_for_verse(verse_id:str, string_to_check:str) -> dict:\n",
    "    \"\"\" This function performs the inner check for a verse in all availiable translations. It is implemented in the check_results() function. \"\"\"\n",
    "    possible_citations = []\n",
    "\n",
    "    for trsl in bip.all_translations:\n",
    "        verse_text = bip.get_verse_text(trsl, verse_id, print_exceptions=False)\n",
    "        \n",
    "        if verse_text:\n",
    "            subverses = bip.split_verse(verse_text, tole_len=21, return_shorts=True, short_limit=9)\n",
    "\n",
    "            fuzzy_matched_subs_num = 0\n",
    "            fuzzy_matched_subs = []\n",
    "            matched_subs_edit_distance = 0\n",
    "            matched_subs_chars = 0\n",
    "            exclusive_matched_subs_num = 0\n",
    "\n",
    "            for subverse in subverses:\n",
    "                # check for every subverse in edit distance\n",
    "                fuzzy_match, query_match, edit_distance = fuzzy_string_matching_for_implementation_with_text(subverse, query_string=string_to_check, tolerance=0.85)\n",
    "                if fuzzy_match:\n",
    "                    fuzzy_matched_subs_num += 1\n",
    "                    fuzzy_matched_subs.append(subverse)\n",
    "                    matched_subs_edit_distance += edit_distance\n",
    "                    matched_subs_chars += len(subverse)\n",
    "\n",
    "                    # run the exclussiveness test\n",
    "                    if exclusiveness_test(subverse, query_match):\n",
    "                        exclusive_matched_subs_num += 1\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if fuzzy_matched_subs_num == 0:\n",
    "                continue\n",
    "            else:\n",
    "                matched_characters = (matched_subs_chars-matched_subs_edit_distance)/matched_subs_chars\n",
    "                matched_subverses_score = fuzzy_matched_subs_num/len(subverses)\n",
    "\n",
    "                match_probability = matched_characters*matched_subverses_score\n",
    "\n",
    "                result_for_trsl = {'verse_id': verse_id,\n",
    "                                    'verse_text': verse_text, \n",
    "                                    'matched_subverses': fuzzy_matched_subs, \n",
    "                                    'query_string': string_to_check, \n",
    "                                    'matched_characters': (matched_subs_chars-matched_subs_edit_distance)/matched_subs_chars, \n",
    "                                    'matched_subverses_score': fuzzy_matched_subs_num/len(subverses),\n",
    "                                    'exclusives_match': exclusive_matched_subs_num/fuzzy_matched_subs_num,\n",
    "                                    'match_probability': match_probability}\n",
    "            \n",
    "                possible_citations.append(result_for_trsl)\n",
    "\n",
    "    # If there are none possible citation (which is weird and it should not happen and it probably means that there are differently split verses in the originally used BibleDataset) return result that are basically False:\n",
    "    if not possible_citations:\n",
    "        result = {'verse_id': verse_id,\n",
    "                    'verse_text': verse_text, \n",
    "                    'matched_subverses': [], \n",
    "                    'query_string': string_to_check, \n",
    "                    'matched_characters': 0, \n",
    "                    'matched_subverses_score': 0,\n",
    "                    'exclusives_match': 0,\n",
    "                    'match_probability': 'FALSE'}\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Now, if the results seem OK, select the best match (translations as such are not evaluated, just select the best result of all possible results)... in this evaluation, we consider the result with most detected subverses as a match, if same then based on the characters, and finally on the exclusiveness test results.\n",
    "    matched_subverses_scores = [pc['matched_subverses'] for pc in possible_citations]\n",
    "    matched_characters_scores = [pc['matched_characters'] for pc in possible_citations]\n",
    "    exclusiveness_test_scores = [pc['exclusives_match'] for pc in possible_citations]\n",
    "\n",
    "    # Check subverses score results:\n",
    "    best_subverses_match = max(matched_subverses_scores)\n",
    "    if matched_subverses_scores.count(best_subverses_match) == 1:\n",
    "        best_pc_idx = matched_subverses_scores.index(best_subverses_match)\n",
    "        return possible_citations[best_pc_idx]\n",
    "    else:\n",
    "        # check the character scores results:\n",
    "        idxs = [i for i, score in enumerate(matched_subverses_scores) if score == best_subverses_match]\n",
    "        best_chars_match = max([matched_characters_scores[i] for i in idxs])\n",
    "        if matched_characters_scores.count(best_chars_match) == 1:\n",
    "            best_pc_idx = matched_characters_scores.index(best_chars_match)\n",
    "            return possible_citations[best_pc_idx]\n",
    "        else:\n",
    "            # check exclusiveness test results:\n",
    "            idxs = [i for i, score in enumerate(matched_characters_scores) if score == best_chars_match]\n",
    "            best_excl_res = max([exclusiveness_test_scores[i] for i in idxs])\n",
    "            best_pc_idx = exclusiveness_test_scores.index(best_excl_res)\n",
    "            return possible_citations[best_pc_idx]\n",
    "\n",
    "\n",
    "def load_data_from_journals_fulldata(journals_fulldata:dict, query_file:str):\n",
    "    journal = journals_fulldata[query_file]['journal']\n",
    "    issue_date = journals_fulldata[query_file]['issue_date']\n",
    "    issue_page = journals_fulldata[query_file]['issue_page']\n",
    "    issue_uuid = journals_fulldata[query_file]['issue_uuid']\n",
    "    kramerius_url = journals_fulldata[query_file]['kramerius_url']\n",
    "    full_query_string = journals_fulldata[query_file]['text']\n",
    "\n",
    "    return journal, issue_date, issue_page, issue_uuid, kramerius_url, full_query_string\n",
    "\n",
    "\n",
    "def evaluate_attributions_in_doc(attributed_verses:dict, query_file:str, query_window_len:int, query_overlap:int, journals_fulldata:dict) -> list:\n",
    "    \"\"\" This function evaluates attributed verses, supposedly detected in a single JSON file. \"\"\"\n",
    "    # Load data from journals_fulldata dictionary:\n",
    "    journal, issue_date, issue_page, issue_uuid, kramerius_url, full_query_string = load_data_from_journals_fulldata(journals_fulldata=journals_fulldata, query_file=query_file)\n",
    "    \n",
    "    # NOTE: repair wrong date with 334149b0-877c-11e6-8aeb-5ef3fc9ae867\n",
    "    if '334149b0-877c-11e6-8aeb-5ef3fc9ae867' in issue_uuid:\n",
    "        issue_date = '30.06.1935'\n",
    "\n",
    "    query_parts = bip.split_query(full_query_string, window_len=query_window_len, overlap=query_overlap)\n",
    "    \n",
    "    results_of_attributions = []\n",
    "  \n",
    "    for verse_id in attributed_verses:\n",
    "        attributed_idxs = attributed_verses[verse_id]\n",
    "        if len(attributed_idxs) == 1:\n",
    "            string_to_check = query_parts[attributed_idxs[0]]\n",
    "            possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "            results_of_attributions.append(possible_citation)\n",
    "            \n",
    "        else:\n",
    "            skip = False\n",
    "            for i, q_idx in enumerate(attributed_idxs):\n",
    "                if not skip:\n",
    "                    try:\n",
    "                        if attributed_idxs[i+1] == q_idx+1:\n",
    "                            # checking if the next part is a joined sequence\n",
    "                            skip = True\n",
    "                            string_to_check = join_overlap(query_parts, q_idx)\n",
    "                            possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "                            results_of_attributions.append(possible_citation)\n",
    "                        else:\n",
    "                            string_to_check = query_parts[attributed_idxs[0]]\n",
    "                            possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "                            results_of_attributions.append(possible_citation)\n",
    "                    except IndexError:\n",
    "                        string_to_check = query_parts[attributed_idxs[i]]\n",
    "                        possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "                        results_of_attributions.append(possible_citation)\n",
    "                else:\n",
    "                    skip = False\n",
    "                    continue\n",
    "\n",
    "    # TODO: zde pak přidat všechny další parametry nalezené citace --> pak se to vrátí a přidá do výsledného DF.\n",
    "    if len(results_of_attributions) == 1:\n",
    "        results_of_attributions[0]['multiple_attribution'] = False\n",
    "        results_of_attributions[0]['journal'] = journal\n",
    "        results_of_attributions[0]['date'] = issue_date\n",
    "        results_of_attributions[0]['page_num'] = issue_page\n",
    "        results_of_attributions[0]['uuid'] = issue_uuid\n",
    "        results_of_attributions[0]['kramerius_url'] = kramerius_url\n",
    "    else:\n",
    "        for res in results_of_attributions:\n",
    "            res['multiple_attribution'] = True\n",
    "            res['journal'] = journal\n",
    "            res['date'] = issue_date\n",
    "            res['page_num'] = issue_page\n",
    "            res['uuid'] = issue_uuid\n",
    "            res['kramerius_url'] = kramerius_url\n",
    "\n",
    "    return results_of_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filtered_search_dataframe(results_filename='UNFILTERED_batch_results.csv', save=True, return_df=False):\n",
    "    \"\"\" This functions applies initial checks on the preliminary results. \"\"\"\n",
    "    # Load results:\n",
    "    print('Loading UNFILTERED results...')\n",
    "    results_dataframe = load_results(results_filename, delimiter=';')\n",
    "\n",
    "    # Load journals_fulldata:\n",
    "    print('Loading journals_fulldata.joblib...')\n",
    "    journals_full_data = joblib.load(os.path.join(ROOT_PATH, 'journals_fulldata.joblib'))\n",
    "\n",
    "    # Create (empty) final results dataframe:\n",
    "    final_results = {}\n",
    "    res_id = 0\n",
    "\n",
    "    print('Running initial filtering...')\n",
    "    rows_to_skip = []\n",
    "    print_progress = 0\n",
    "    iter_ = 0\n",
    "    for row_id in results_dataframe.index:\n",
    "        print_progress += 1\n",
    "        iter_ += 1\n",
    "        if print_progress >= 500:\n",
    "            print('\\t', iter_, '/', len(results_dataframe))\n",
    "            print_progress = 0\n",
    "\n",
    "        if row_id in rows_to_skip:\n",
    "            continue\n",
    "        else:\n",
    "            query_file, query_window_len, query_overlap = get_row_data_for_initial_filter(dataframe=results_dataframe, row_id=row_id)\n",
    "            attributed_verses, add_to_skip = select_attributions_to_json(dataframe=results_dataframe, query_file=query_file)\n",
    "            rows_to_skip.extend(add_to_skip)\n",
    "\n",
    "            results = evaluate_attributions_in_doc(attributed_verses=attributed_verses, query_file=query_file, query_window_len=query_window_len, query_overlap=query_overlap, journals_fulldata=journals_full_data)\n",
    "\n",
    "            for res in results:\n",
    "                final_results[res_id] = res\n",
    "                res_id += 1\n",
    "\n",
    "    final_results_df = pd.DataFrame.from_dict(final_results)\n",
    "    final_results_df = final_results_df.transpose()\n",
    "    \n",
    "    if save:\n",
    "        final_results_df.to_csv(os.path.join(RESULTS_PATH, f'FILTERED_{results_filename}'), encoding='utf-8', quotechar='\"', sep=';')\n",
    "\n",
    "    if return_df:\n",
    "        return final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bip.make_filtered_search_dataframe(results_filename='UNFILTERED_batch_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Filtering stop-subverses.\n",
    "- define evaluation stop subverses in [evaluation_stop_subverse_21.txt](https://github.com/DigilabNLCR/BibleCitations/blob/main/evaluation_stop_subverses_21.txt) ... the structure of this file must reflect the structure of matched_subverses column of the result dataset. I.e., ['subverse text']\n",
    "- During this step, the citations that include only the subverses defined in [evaluation_stop_subverse_21.txt](https://github.com/DigilabNLCR/BibleCitations/blob/main/evaluation_stop_subverses_21.txt) are ignored.\n",
    "- You should choose the subverses to filter wisely, some of the subverses that seem not worty may indeed be true bible quotes.\n",
    "- In addition, there is [100_hit_needed_subs_21.txt](https://github.com/DigilabNLCR/BibleCitations/blob/main/100_hit_needed_subs_21.txt) file that includes those verses that need to be cited in full (the string of subverse must exactly match the passage where it is supposed to be cited, i.e. no \"typos\" allowed)\n",
    "--> creates 'ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n",
    "--> + creates 'FILTERED_BY_STOP_SUBS.csv' file (so you can check the filtered out citations, too)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pokračovat zde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Drop 'hidden' duplicates\n",
    "These are the duplicate results that have formally different query string, but actually one of the query strings contains the other (this means that there was an overlap in the split of query document, most likely in the end of the query documents)\n",
    "--> creates 'DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Marking multiple attributions\n",
    "In this case the multiple attributions are not dropped, but kept with a column that suggest if the result should be dropped or not.\n",
    "--> creates 'MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Marking 'sure' citations\n",
    "- We consider a 'sure' citation one that is cited DOPLNIT\n",
    "--> creates 'FINAL_MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv' file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_verses = {\n",
    "    'L 11:3/Mt 6:11': ['L 11:3', 'Mt 6:11'],\n",
    "    'Mk 13:31/Mt 24:35/L 21:33': ['Mk 13:31', 'Mt 24:35', 'L 21:33'],\n",
    "    'Ex 20:16/Dt 5:20': ['Ex 20:16', 'Dt 5:20'],\n",
    "    '2K 1:2/Fp 1:2/2Te 1:2/1K 1:3/Ef 1:2/Ga 1:3': ['2K 1:2', 'Fp 1:2', '2Te 1:2', '1K 1:3', 'Ef 1:2', 'Ga 1:3'],\n",
    "    'Mt 11:15/Mt 13:9': ['Mt 11:15', 'Mt 13:9']\n",
    "    \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8427d73f02270cccde5d7b657eb9efbd549974ced0a163dd64bdafb1c87004cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
