{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_biblical_intertextuality as bip\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Defining paths. \"\"\"\n",
    "ROOT_PATH = os.getcwd()\n",
    "\n",
    "BIBLES_PATH = os.path.join(ROOT_PATH, 'Bible_files')\n",
    "QUERY_DOC_PATH = os.path.join(ROOT_PATH, 'query_documents')\n",
    "DATASETS_PATH = os.path.join(ROOT_PATH, 'datasets')\n",
    "DICTS_PATH = os.path.join(ROOT_PATH, 'dictionaries')\n",
    "CORPUS_PATH = os.path.join(ROOT_PATH, 'corpuses')\n",
    "RESULTS_PATH = os.path.join(ROOT_PATH, 'results')\n",
    "ALL_JSONS_PATH = os.path.join(ROOT_PATH, 'extracted_query_jsons')\n",
    "\n",
    "BATCHES_FILE_PATH = os.path.join(ROOT_PATH, 'batches.csv')\n",
    "BATCH_RESULTS_FILE_PATH = os.path.join(RESULTS_PATH, 'batch_results.csv')\n",
    "\n",
    "STOP_WORDS_PATH = os.path.join(ROOT_PATH, 'stop_words.txt')\n",
    "STOP_SUBVERSES_PATH = os.path.join(ROOT_PATH, 'stop_subverses_21.txt')\n",
    "EXCLUSIVES_PATH = os.path.join(ROOT_PATH, 'exclusives.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Přišel zajisté Syn člověka', 'aby spasil to což bylo zahynulo.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bip.split_verse(bip.get_verse_text('BKR', 'Mt 18:11'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(results_filename='batch_results.csv') -> pd.core.frame.DataFrame:\n",
    "    \"\"\" This function loads selected results from the results folder. It is returned as pandas dataframe\n",
    "    \n",
    "    :param results_filename: filename of results; 'batch_results.csv' is the default parameter, as this is the default filename of results from the search functions.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['verse_id', 'query_file', 'index_query_part', 'batch_id', 'ngram_size','query_window_len', 'query_overlap', 'ngram_tolerance', 'edit_distance_tolerance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_data(dataframe:pd.core.frame.DataFrame, row_id:int):\n",
    "    \"\"\" This function returns search properties of a given row in the results dataframe. \"\"\"\n",
    "\n",
    "    verse_id = dataframe.loc[row_id]['verse_id']\n",
    "    query_file = dataframe.loc[row_id]['query_file']\n",
    "    index_query_part = dataframe.loc[row_id]['index_query_part']\n",
    "    batch_id = dataframe.loc[row_id]['batch_id']\n",
    "    ngram_size = dataframe.loc[row_id]['ngram_size']\n",
    "    query_window_len = dataframe.loc[row_id]['query_window_len']\n",
    "    query_overlap = dataframe.loc[row_id]['query_overlap']\n",
    "    ngram_tolerance = dataframe.loc[row_id]['ngram_tolerance']\n",
    "    edit_distance_tolerance = dataframe.loc[row_id]['edit_distance_tolerance']\n",
    "\n",
    "    return verse_id, query_file, index_query_part, batch_id, ngram_size, query_window_len, query_overlap, ngram_tolerance, edit_distance_tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verse_et_idx(dataframe:pd.core.frame.DataFrame, row_id:int):\n",
    "    \"\"\" This function returns search properties of a given row in the results dataframe. \"\"\"\n",
    "\n",
    "    verse_id = dataframe.loc[row_id, 'verse_id']\n",
    "    index_query_part = dataframe.loc[row_id, 'index_query_part']\n",
    "\n",
    "    return verse_id, index_query_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_overlap(list_of_parts:list, query_index:int) -> str:\n",
    "    \"\"\" This function serves to join two parts of a query into one string (when the citation has been discovered in two consecutive parts of the query document). \"\"\"\n",
    "    output = ''\n",
    "\n",
    "    sentences_in_1 = sent_tokenize(list_of_parts[query_index])\n",
    "    try:\n",
    "        sentences_in_2 = sent_tokenize(list_of_parts[query_index+1])\n",
    "    except IndexError:\n",
    "        print(sentences_in_1)\n",
    "        print(list_of_parts[-1])\n",
    "\n",
    "    for sent_1 in sentences_in_1:\n",
    "        if sent_1 not in sentences_in_2:\n",
    "            output += sent_1 + ' '\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for sent_2 in sentences_in_2:\n",
    "        output += sent_2 + ' '\n",
    "\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_string_matching_for_implementation_with_text(subverse_string:str, query_string:str, tolerance=0.85):\n",
    "    \"\"\" \n",
    "    Contrary to fuzzy_string_matching_for_implementation(), this function also returns the matched part of the query string and the edit distance of the compared strings. The function is duplicated so as not to speed down the function in the broad search. However, the speed difference has not been tested yet.\n",
    "\n",
    "    This function is for implementation of typo similarity detection applied to two strings. It returns bool value of match.\n",
    "\n",
    "    :param subverse_string: string of the biblical subverse we are searching for.\n",
    "    :param query_string: string in which we are searching for the seubverse_string.\n",
    "    :param tolerance: how large proportion of the subverse_string must be present in query_string to consider it a match.\n",
    "    \"\"\"\n",
    "    subverse_string = bip.normalize_string(subverse_string)\n",
    "    subverse_len = len(subverse_string)\n",
    "\n",
    "    query_string = bip.normalize_string(query_string)\n",
    "    query_len = len(query_string)\n",
    "\n",
    "    tolerance = subverse_len * (1-tolerance)\n",
    "\n",
    "    if subverse_len-tolerance > query_len:\n",
    "        # If subverse is longer than query string, it is not a match by default\n",
    "        return False, '', 0\n",
    "    elif subverse_len-tolerance <= query_len <= subverse_len+tolerance:\n",
    "        # If subverse is more or les of the same length as query string, just compare them.\n",
    "        edit_distance = Levenshtein.distance(subverse_string, query_string)\n",
    "        if edit_distance <= tolerance:\n",
    "            return True, query_string, edit_distance\n",
    "    else:\n",
    "        # Oherwise, compare parts of the query string (always staring with word, so it is quicker; however, some mistakes may be made here.\n",
    "        word_len_subv = len(word_tokenize(subverse_string))\n",
    "        words_in_query_string = word_tokenize(query_string)\n",
    "        word_len_query_string = len(words_in_query_string)\n",
    "\n",
    "        for i, cycle in enumerate(range(word_len_subv, word_len_query_string+1)):\n",
    "            gram_str = ' '.join(words_in_query_string[i:(word_len_subv+i)])\n",
    "            edit_distance = Levenshtein.distance(subverse_string, gram_str)\n",
    "            if edit_distance <= tolerance:\n",
    "                return True, gram_str, edit_distance\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    return False, '', 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define mutually exclusive words in exclusives.txt \"\"\"\n",
    "with open(EXCLUSIVES_PATH, 'r', encoding='utf-8') as exclusives_file:\n",
    "    data = exclusives_file.read()\n",
    "    words_lines = data.split('\\n')\n",
    "\n",
    "    exclusives_dict = defaultdict(list)\n",
    "    list_of_exclusives = []\n",
    "\n",
    "    for line in words_lines:\n",
    "        word_list = line.split(', ')\n",
    "        for word_fro in word_list:\n",
    "            for word_to in word_list:\n",
    "                if word_fro == word_to:\n",
    "                    continue\n",
    "                if word_fro == 'je' and word_to == 'jest':\n",
    "                    continue\n",
    "                if word_fro == 'jest' and word_to == 'je':\n",
    "                    continue\n",
    "                else:\n",
    "                    exclusives_dict[bip.normalize_string(word_fro)].append(bip.normalize_string(word_to))\n",
    "            list_of_exclusives.append(bip.normalize_string(word_fro))\n",
    "\n",
    "def exclusiveness_test(subverse_string:str, query_string:str) -> bool:\n",
    "    \"\"\"\n",
    "    This function serves to check if the detected string is not false positive based on mutually exclusive words. E.g. naše vs. vaše;, je vs, není etc.\n",
    "    \"\"\"\n",
    "    subverse_string = bip.normalize_string(subverse_string)\n",
    "    query_string = bip.normalize_string(query_string)\n",
    "\n",
    "    subverse_words = bip.word_tokenize_no_punctuation(subverse_string)\n",
    "    query_words = bip.word_tokenize_no_punctuation(query_string)\n",
    "\n",
    "    for i, word in enumerate(subverse_words):\n",
    "        if word in list_of_exclusives:\n",
    "            list_to_ex = exclusives_dict[word]\n",
    "            try:\n",
    "                if query_words[i] in list_to_ex:\n",
    "                    return False\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_next_result_continuation(results_dataframe, row_id:int) -> bool:\n",
    "    \"\"\" This function checks if the next result is just the continuation of the previous, so as to check them together. \"\"\"\n",
    "    try:\n",
    "        query_filename = results_dataframe.loc[row_id, 'query_file']\n",
    "        verse_id = results_dataframe.loc[row_id, 'verse_id']\n",
    "        query_idx = int(results_dataframe.loc[row_id, 'index_query_part'])\n",
    "        \n",
    "        next_query_filename = results_dataframe.loc[row_id+1, 'query_file']\n",
    "        next_verse_id = results_dataframe.loc[row_id+1, 'verse_id']\n",
    "        next_query_idx = int(results_dataframe.loc[row_id+1, 'index_query_part'])\n",
    "\n",
    "        if query_filename == next_query_filename and verse_id == next_verse_id and query_idx == next_query_idx-1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_multiple_attribution(results_dataframe, row_id:int):\n",
    "    \"\"\" Checks if the same part of a query is associated with multiple verse IDs. \"\"\"\n",
    "    query_filename = results_dataframe.loc[row_id, 'query_file']\n",
    "    verse_id = results_dataframe.loc[row_id, 'verse_id']\n",
    "    query_idx = results_dataframe.loc[row_id, 'index_query_part']\n",
    "\n",
    "    other_attributions_df = results_dataframe[results_dataframe['query_file'] == query_filename]\n",
    "    other_attributions_df = other_attributions_df[other_attributions_df['index_query_part'] == query_idx]\n",
    "\n",
    "    other_associated_verses = []\n",
    "    rows_to_skip = []\n",
    "    for row_id in other_attributions_df.index:\n",
    "        other_associated_verses.append(other_attributions_df.loc[row_id, 'verse_id'])\n",
    "        # rows_to_skip.append((query_filename, verse_id, query_idx+1))\n",
    "        rows_to_skip.append((query_filename, verse_id, query_idx))\n",
    "\n",
    "    return other_associated_verses, rows_to_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_probability(subverses:list, fuzzy_matched_subs_num:int, exclusive_matched_subs_num: int, matched_subs_chars:int, matched_subs_edit_distance:int) -> str:\n",
    "    \"\"\" Returns string that express match probability. Implemented in check_results() function. \"\"\"\n",
    "    matched_characters = (matched_subs_chars-matched_subs_edit_distance)/matched_subs_chars\n",
    "    matched_subverses_score = fuzzy_matched_subs_num/len(subverses)\n",
    "    exclusives_match = exclusive_matched_subs_num/fuzzy_matched_subs_num\n",
    "\n",
    "    if exclusives_match == 1:\n",
    "        if matched_characters >= 0.9:\n",
    "            if matched_subverses_score == 1:\n",
    "                return 'VERSE MATCH'\n",
    "            else:\n",
    "                return 'SUBVERSE MATCH'\n",
    "        else:\n",
    "            if matched_subverses_score == 1:\n",
    "                return 'POSSIBLE VERSE MATCH'\n",
    "            else:\n",
    "                return 'POSSIBLE SUBVERSE MATCH'\n",
    "\n",
    "    elif exclusives_match == 0:\n",
    "        if matched_characters >= 0.9:\n",
    "            if matched_subverses_score == 1:\n",
    "                return 'EXCLUSIVES ERROR, VERSE MATCH'\n",
    "            else:\n",
    "                return 'EXCLUSIVES ERROR, SUBVERSE MATCH'\n",
    "        else:\n",
    "            if matched_subverses_score == 1:\n",
    "                return 'EXCLUSIVES ERROR, POSSIBLE VERSE MATCH'\n",
    "            else:\n",
    "                return 'EXCLUSIVES ERROR, POSSIBLE SUBVERSE MATCH'\n",
    "\n",
    "    else:\n",
    "        if matched_characters >= 0.9:\n",
    "            if matched_subverses_score == 1:\n",
    "                return 'EXCLUSIVES PROBLEM, VERSE MATCH'\n",
    "            else:\n",
    "                return 'EXCLUSIVES PROBLEM, SUBVERSE MATCH'\n",
    "        else:\n",
    "            if matched_subverses_score == 1:\n",
    "                return 'EXCLUSIVES PROBLEM, POSSIBLE VERSE MATCH'\n",
    "            else:\n",
    "                return 'EXCLUSIVES PROBLEM, POSSIBLE SUBVERSE MATCH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_verse_in_translations(verse_id:str, string_to_check:str, journal:str, issue_date:str, issue_page_num:str, issue_uuid:str, kramerius_url:str) -> list:\n",
    "    \"\"\" This function performs the inner check for a verse in all availiable translations. It is implemented in the check_results() function. \"\"\"\n",
    "    possible_citations = []\n",
    "\n",
    "    for trsl in bip.all_translations:\n",
    "        verse_text = bip.get_verse_text(trsl, verse_id, print_exceptions=False)\n",
    "        if verse_text:\n",
    "            subverses = bip.split_verse(verse_text, tole_len=21)\n",
    "\n",
    "            fuzzy_matched_subs_num = 0\n",
    "            fuzzy_matched_subs = []\n",
    "            matched_subs_edit_distance = 0\n",
    "            matched_subs_chars = 0\n",
    "            exclusive_matched_subs_num = 0\n",
    "\n",
    "            for subverse in subverses:\n",
    "                # check for every subverse in edit distance\n",
    "                fuzzy_match, query_match, edit_distance = fuzzy_string_matching_for_implementation_with_text(subverse, query_string=string_to_check, tolerance=0.85)\n",
    "                if fuzzy_match:\n",
    "                    fuzzy_matched_subs_num += 1\n",
    "                    fuzzy_matched_subs.append(subverse)\n",
    "                    matched_subs_edit_distance += edit_distance\n",
    "                    matched_subs_chars += len(subverse)\n",
    "\n",
    "                    # run the exclussiveness test\n",
    "                    if exclusiveness_test(subverse, query_match):\n",
    "                        exclusive_matched_subs_num += 1\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if fuzzy_matched_subs_num == 0:\n",
    "                continue\n",
    "            else:            \n",
    "                match_probability = get_match_probability(subverses, fuzzy_matched_subs_num, exclusive_matched_subs_num, matched_subs_chars, matched_subs_edit_distance)\n",
    "\n",
    "                result_for_trsl = {'verse_id': verse_id, \n",
    "                                    'translation': trsl, \n",
    "                                    'verse_text': verse_text, \n",
    "                                    'matched_subvesrses': fuzzy_matched_subs, \n",
    "                                    'query_string': string_to_check, \n",
    "                                    'matched_characters': (matched_subs_chars-matched_subs_edit_distance)/matched_subs_chars, \n",
    "                                    'matched_subverses_score': fuzzy_matched_subs_num/len(subverses),\n",
    "                                    'exclusives_match': exclusive_matched_subs_num/fuzzy_matched_subs_num,\n",
    "                                    'multiple_attribution': 'No',\n",
    "                                    'match_probability': match_probability,\n",
    "                                    'journal': journal, \n",
    "                                    'date': issue_date, \n",
    "                                    'page_num': issue_page_num, \n",
    "                                    'uuid': issue_uuid, \n",
    "                                    'kramerius_url': kramerius_url}\n",
    "            \n",
    "                possible_citations.append(result_for_trsl)\n",
    "\n",
    "    return possible_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_citations(possible_citations:list) -> list:\n",
    "    \"\"\" This function selects the best match from the given list based on its score. If more verse IDs/translations score the same, all are counted in. \"\"\"\n",
    "    # TODO: Některé citace jsou vyřazeny zbytečně - protože v jedné query sekvenci může být více citací!!! Takže asi spíše vyřazovat jen ty co mají hodně nízké skóre, exclusive error... nebo je možné nějak zjišťovat, jestli se ty citace nacházejí v jiných částech query dokumentu, ale to je možná zbytečně problematické ... nejlepší bude asi zvěrečná ruční kontrola!\n",
    "    \n",
    "    # different match probabilities are differently valued\n",
    "    match_probability_scores ={'VERSE MATCH': 100,\n",
    "                               'POSSIBLE VERSE MATCH': 90,\n",
    "                               'SUBVERSE MATCH': 90,                          \n",
    "                               'POSSIBLE SUBVERSE MATCH': 80,\n",
    "                               'EXCLUSIVES PROBLEM, VERSE MATCH': 85, \n",
    "                               'EXCLUSIVES PROBLEM, SUBVERSE MATCH': 70,\n",
    "                               'EXCLUSIVES PROBLEM, POSSIBLE VERSE MATCH': 70,\n",
    "                               'EXCLUSIVES PROBLEM, POSSIBLE SUBVERSE MATCH': 60,\n",
    "                               'EXCLUSIVES ERROR, VERSE MATCH': 60,\n",
    "                               'EXCLUSIVES ERROR, SUBVERSE MATCH': 30,\n",
    "                               'EXCLUSIVES ERROR, POSSIBLE VERSE MATCH': 10,\n",
    "                               'EXCLUSIVES ERROR, POSSIBLE SUBVERSE MATCH': 0}\n",
    "    \n",
    "    scores = []\n",
    "    for pos_cit in possible_citations:\n",
    "        characters_score = pos_cit['matched_characters']*50\n",
    "        subverses_score = pos_cit['matched_subverses_score']*50\n",
    "        exclusive_score = pos_cit['exclusives_match']*20\n",
    "        probability_score = match_probability_scores[pos_cit['match_probability']]\n",
    "\n",
    "        scores.append(characters_score + subverses_score + exclusive_score + probability_score)\n",
    "\n",
    "    top_score = max(scores)\n",
    "\n",
    "    best_citations = []\n",
    "    for i, score in enumerate(scores):\n",
    "        if score == top_score:\n",
    "            if possible_citations[i] not in best_citations:\n",
    "                best_citations.append(possible_citations[i])\n",
    "\n",
    "    if len(best_citations) > 1:\n",
    "        for bc in best_citations:\n",
    "            bc['multiple_attribution'] = 'Yes'\n",
    "    \n",
    "    return best_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_from_json(json_path:str):\n",
    "    \"\"\" This function loads all needed data from selected JSON file. \"\"\"\n",
    "    data = bip.load_json_data(json_path)\n",
    "    journal = data['journal']\n",
    "    issue_date = data['date']\n",
    "    issue_page = data['page_num']\n",
    "    issue_uuid = data['issue_uuid']\n",
    "    kramerius_url = f'https://kramerius5.nkp.cz/view/uuid:{issue_uuid}'\n",
    "    full_query_string = data['text']\n",
    "\n",
    "    return journal, issue_date, issue_page, issue_uuid, kramerius_url, full_query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_to_json_file(filename:str):\n",
    "    \"\"\" This function returns the path to a specific json file based the filename. It uses the batches.csv file for this.\"\"\"\n",
    "    batches_df = pd.read_csv(BATCHES_FILE_PATH, quotechar='\"', delimiter=',', encoding='utf-8')\n",
    "\n",
    "    subset_df = batches_df.loc[batches_df['json_file'] == filename]\n",
    "\n",
    "    path_to_json = os.path.join(ALL_JSONS_PATH, subset_df.iloc[0]['journal'], filename)\n",
    "\n",
    "    return path_to_json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: DROP THIS? IT WILL NOT BE USED\n",
    "\n",
    "final_dataframe_column_names = ['verse_id', 'translation', 'verse_text', 'matched_subvesrses', 'query_string', 'matched_characters', 'matched_subverses_score', 'exclusives_match',  'multiple_attribution', 'match_probability', 'journal', 'date', 'page_num', 'uuid', 'kramerius_url']\n",
    "\n",
    "def check_results(results_filename='batch_results.csv'):\n",
    "    \"\"\" This functions applies further checks on the preliminary results. \"\"\"\n",
    "    # Load results:\n",
    "    results_dataframe = load_results(results_filename)\n",
    "\n",
    "    # Remove duplicate rows from the result dataframe\n",
    "    print('Original size of the results dataframe:', len(results_dataframe))\n",
    "    results_dataframe.drop_duplicates(subset=['verse_id', 'query_file', 'index_query_part'], keep='first', inplace=True)\n",
    "    print('Size of the results dataframe after droping duplicates:', len(results_dataframe))\n",
    "\n",
    "    # Create (empty) final results dataframe:\n",
    "    final_results = pd.DataFrame(columns=final_dataframe_column_names)\n",
    "\n",
    "    rows_to_skip = []\n",
    "    for row_id in results_dataframe.index:\n",
    "        verse_id, query_filename, query_idx, batch_id, ngram_size, query_window_len, query_overlap, ngram_tolerance, edit_distance_tolerance = get_row_data(dataframe=results_dataframe, row_id=row_id)\n",
    "  \n",
    "        if (query_filename, verse_id, query_idx) in rows_to_skip:\n",
    "            continue\n",
    "        else:\n",
    "            # If the file is differnet than the previous_file, we need to load the data\n",
    "            json_path = get_path_to_json_file(query_filename)\n",
    "\n",
    "            journal, issue_date, issue_page, issue_uuid, kramerius_url, full_query_string = load_metadata_from_json(json_path=json_path)\n",
    "            query_parts = bip.split_query(full_query_string, window_len=query_window_len, overlap=query_overlap)\n",
    "            \n",
    "            if is_next_result_continuation(results_dataframe, row_id):\n",
    "                # Check if the next result is just a continuation of the current one and if so, join them to one.\n",
    "                string_to_check = join_overlap(query_parts, query_idx)\n",
    "                rows_to_skip.append((query_filename, verse_id, query_idx+1))\n",
    "            else:\n",
    "                string_to_check = query_parts[query_idx]\n",
    "\n",
    "            associated_verses, new_rows_to_skip = has_multiple_attribution(results_dataframe, row_id)\n",
    "\n",
    "            rows_to_skip.extend(new_rows_to_skip)\n",
    "            possible_citations = []\n",
    "            for asoc_verse_id in associated_verses:\n",
    "                asoc_possible_citations = check_for_verse_in_translations(asoc_verse_id, string_to_check, journal, issue_date, issue_page, issue_uuid, kramerius_url)\n",
    "                possible_citations.extend(asoc_possible_citations)\n",
    "\n",
    "            best_citations = select_best_citations(possible_citations)\n",
    "\n",
    "            for bc in best_citations:\n",
    "                final_results = final_results.append(bc, ignore_index=True)\n",
    "\n",
    "    # Filter duplicates in df (same citations in one page):\n",
    "    final_results = final_results.drop_duplicates(subset=['verse_id', 'translation', 'matched_characters', 'matched_subverses_score', 'page_num', 'uuid'])\n",
    "    \n",
    "    # Save filtered_results:\n",
    "    output_filename = f'FILTERED_{results_filename}'\n",
    "    final_results.to_csv(os.path.join(RESULTS_PATH, output_filename), sep=';', quotechar='\"', encoding='utf-8')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_attributions_to_json(dataframe:pd.core.frame.DataFrame, query_file:str):\n",
    "    \"\"\" This function selects all attributions to a given JSON file. \n",
    "    \n",
    "    It returns: dataframe of all of the results, row_ids to skip\n",
    "    \"\"\"\n",
    "    subset_dataframe = dataframe[dataframe['query_file'] == query_file]\n",
    "\n",
    "    # If the subset dataframe contains only one result, return it and empty skips.\n",
    "    if len(subset_dataframe) == 1:\n",
    "        verse_id, index_query_part = get_verse_et_idx(subset_dataframe, subset_dataframe.index[0])\n",
    "        attributed_verses = {verse_id: [index_query_part]}\n",
    "        return attributed_verses, []\n",
    "\n",
    "    # If the subset dataframe contains more rows, check if further.\n",
    "    else:\n",
    "        row_ids_to_skip = subset_dataframe.index\n",
    "        attributed_verses = defaultdict(list)\n",
    "        for row_id in row_ids_to_skip:\n",
    "            verse_id, index_query_part = get_verse_et_idx(dataframe=subset_dataframe, row_id=row_id)\n",
    "            attributed_verses[verse_id].append(index_query_part)\n",
    "\n",
    "        return attributed_verses, row_ids_to_skip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_verse(verse_id:str, string_to_check:str) -> dict:\n",
    "    \"\"\" This function performs the inner check for a verse in all availiable translations. It is implemented in the check_results() function. \"\"\"\n",
    "    possible_citations = []\n",
    "\n",
    "    for trsl in bip.all_translations:\n",
    "        verse_text = bip.get_verse_text(trsl, verse_id, print_exceptions=False)\n",
    "        if verse_text:\n",
    "            subverses = bip.split_verse(verse_text, tole_len=21)\n",
    "\n",
    "            fuzzy_matched_subs_num = 0\n",
    "            fuzzy_matched_subs = []\n",
    "            matched_subs_edit_distance = 0\n",
    "            matched_subs_chars = 0\n",
    "            exclusive_matched_subs_num = 0\n",
    "\n",
    "            for subverse in subverses:\n",
    "                # check for every subverse in edit distance\n",
    "                fuzzy_match, query_match, edit_distance = fuzzy_string_matching_for_implementation_with_text(subverse, query_string=string_to_check, tolerance=0.85)\n",
    "                if fuzzy_match:\n",
    "                    fuzzy_matched_subs_num += 1\n",
    "                    fuzzy_matched_subs.append(subverse)\n",
    "                    matched_subs_edit_distance += edit_distance\n",
    "                    matched_subs_chars += len(subverse)\n",
    "\n",
    "                    # run the exclussiveness test\n",
    "                    if exclusiveness_test(subverse, query_match):\n",
    "                        exclusive_matched_subs_num += 1\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if fuzzy_matched_subs_num == 0:\n",
    "                continue\n",
    "            else:\n",
    "                matched_characters = (matched_subs_chars-matched_subs_edit_distance)/matched_subs_chars\n",
    "                matched_subverses_score = fuzzy_matched_subs_num/len(subverses)\n",
    "\n",
    "                match_probability = matched_characters*matched_subverses_score\n",
    "\n",
    "                result_for_trsl = {'verse_id': verse_id,\n",
    "                                    'verse_text': verse_text, \n",
    "                                    'matched_subverses': fuzzy_matched_subs, \n",
    "                                    'query_string': string_to_check, \n",
    "                                    'matched_characters': (matched_subs_chars-matched_subs_edit_distance)/matched_subs_chars, \n",
    "                                    'matched_subverses_score': fuzzy_matched_subs_num/len(subverses),\n",
    "                                    'exclusives_match': exclusive_matched_subs_num/fuzzy_matched_subs_num,\n",
    "                                    'match_probability': match_probability}\n",
    "            \n",
    "                possible_citations.append(result_for_trsl)\n",
    "\n",
    "    # Now, select the best match (translations as such are not evaluated, just select the best result of all possible results)... in this evaluation, we consider the result with most detected subverses as a match, if same then based on the characters, and finally on the exclusiveness test results.\n",
    "    matched_subverses_scores = [pc['matched_subverses'] for pc in possible_citations]\n",
    "    matched_characters_scores = [pc['matched_characters'] for pc in possible_citations]\n",
    "    exclusiveness_test_scores = [pc['exclusives_match'] for pc in possible_citations]\n",
    "\n",
    "    # check subverses score results:\n",
    "    best_subverses_match = max(matched_subverses_scores)\n",
    "    if matched_subverses_scores.count(best_subverses_match) == 1:\n",
    "        best_pc_idx = matched_subverses_scores.index(best_subverses_match)\n",
    "        return possible_citations[best_pc_idx]\n",
    "    else:\n",
    "        # check the character scores results:\n",
    "        idxs = [i for i, score in enumerate(matched_subverses_scores) if score == best_subverses_match]\n",
    "        best_chars_match = max([matched_characters_scores[i] for i in idxs])\n",
    "        if matched_characters_scores.count(best_chars_match) == 1:\n",
    "            best_pc_idx = matched_characters_scores.index(best_chars_match)\n",
    "            return possible_citations[best_pc_idx]\n",
    "        else:\n",
    "            # check exclusiveness test results:\n",
    "            idxs = [i for i, score in enumerate(matched_characters_scores) if score == best_chars_match]\n",
    "            best_excl_res = max([exclusiveness_test_scores[i] for i in idxs])\n",
    "            best_pc_idx = exclusiveness_test_scores.index(best_excl_res)\n",
    "            return possible_citations[best_pc_idx]\n",
    "\n",
    "# TODO: TUTO FCI ověřit na R 13:1 a na uuid 0ee796e0-897b-11e6-84e2-005056827e51 (page 9) ... zdá se mi, že to detekovalo slabší verzi verše!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attributions_in_doc(attributed_verses:dict, query_file:str, query_window_len:int, query_overlap:int) -> list:\n",
    "    \"\"\" This function evaluates attributed verses, supposedly detected in a single JSON file. \"\"\"\n",
    "    # Load data from JSON file:\n",
    "    path_to_json = get_path_to_json_file(query_file)\n",
    "    journal, issue_date, issue_page, issue_uuid, kramerius_url, full_query_string = load_metadata_from_json(json_path=path_to_json)\n",
    "\n",
    "    query_parts = bip.split_query(full_query_string, window_len=query_window_len, overlap=query_overlap)\n",
    "    \n",
    "    results_of_attributions = []\n",
    "  \n",
    "    for verse_id in attributed_verses:\n",
    "        attributed_idxs = attributed_verses[verse_id]\n",
    "        if len(attributed_idxs) == 1:\n",
    "            string_to_check = query_parts[attributed_idxs[0]]\n",
    "            possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "            results_of_attributions.append(possible_citation)\n",
    "            \n",
    "        else:\n",
    "            skip = False\n",
    "            for i, q_idx in enumerate(attributed_idxs):\n",
    "                if not skip:\n",
    "                    try:\n",
    "                        if attributed_idxs[i+1] == q_idx+1:\n",
    "                            # checking if the next part is a joined sequence\n",
    "                            skip = True\n",
    "                            string_to_check = join_overlap(query_parts, q_idx)\n",
    "                            possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "                            results_of_attributions.append(possible_citation)\n",
    "                        else:\n",
    "                            string_to_check = query_parts[attributed_idxs[0]]\n",
    "                            possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "                            results_of_attributions.append(possible_citation)\n",
    "                    except IndexError:\n",
    "                        string_to_check = query_parts[attributed_idxs[i]]\n",
    "                        possible_citation = check_for_verse(verse_id=verse_id, string_to_check=string_to_check)\n",
    "                        results_of_attributions.append(possible_citation)\n",
    "                else:\n",
    "                    skip = False\n",
    "                    continue\n",
    "\n",
    "    # TODO: zde pak přidat všechny další parametry nalezené citace --> pak se to vrátí a přidá do výsledného DF.\n",
    "    if len(results_of_attributions) == 1:\n",
    "        results_of_attributions[0]['multiple_attribution'] = False\n",
    "        results_of_attributions[0]['journal'] = journal\n",
    "        results_of_attributions[0]['date'] = issue_date\n",
    "        results_of_attributions[0]['page_num'] = issue_page\n",
    "        results_of_attributions[0]['uuid'] = issue_uuid\n",
    "        results_of_attributions[0]['kramerius_url'] = kramerius_url\n",
    "    else:\n",
    "        for res in results_of_attributions:\n",
    "            res['multiple_attribution'] = True\n",
    "            res['journal'] = journal\n",
    "            res['date'] = issue_date\n",
    "            res['page_num'] = issue_page\n",
    "            res['uuid'] = issue_uuid\n",
    "            res['kramerius_url'] = kramerius_url\n",
    "\n",
    "    return results_of_attributions\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe_column_names = ['verse_id', 'verse_text', 'matched_subverses', 'query_string', 'matched_characters', 'matched_subverses_score', 'exclusives_match',  'multiple_attribution', 'match_probability', 'journal', 'date', 'page_num', 'uuid', 'kramerius_url']\n",
    "\n",
    "\n",
    "def check_results_2(results_filename='batch_results.csv', save=True, return_df=False):\n",
    "    \"\"\" This functions applies further checks on the preliminary results. \"\"\"\n",
    "    # Load results:\n",
    "    results_dataframe = load_results(results_filename)\n",
    "\n",
    "    # Remove duplicate rows from the result dataframe\n",
    "    print('Original size of the results dataframe:', len(results_dataframe))\n",
    "    results_dataframe.drop_duplicates(subset=['verse_id', 'query_file', 'index_query_part'], keep='first', inplace=True)\n",
    "    print('Size of the results dataframe after droping duplicates:', len(results_dataframe))\n",
    "\n",
    "    # Create (empty) final results dataframe:\n",
    "    final_results = {}\n",
    "    res_id = 0\n",
    "\n",
    "    rows_to_skip = []\n",
    "    print_progress = 0\n",
    "    iter_ = 0\n",
    "    for row_id in results_dataframe.index:\n",
    "        print_progress += 1\n",
    "        iter_ += 1\n",
    "        if print_progress >= 500:\n",
    "            print(iter_, '/', len(results_dataframe))\n",
    "            print_progress = 0\n",
    "\n",
    "        # TODO: udělat fci, která nebude kontrolovat tohle všechno, je to celkem zbytečný a nějaký vteřinky to asi ubírá...\n",
    "        verse_id, query_file, index_query_part, batch_id, ngram_size, query_window_len, query_overlap, ngram_tolerance, edit_distance_tolerance = get_row_data(dataframe=results_dataframe, row_id=row_id)\n",
    "  \n",
    "        if row_id in rows_to_skip:\n",
    "            continue\n",
    "        else:\n",
    "            attributed_verses, add_to_skip = select_attributions_to_json(dataframe=results_dataframe, query_file=query_file)\n",
    "            rows_to_skip.extend(add_to_skip)\n",
    "\n",
    "            results = evaluate_attributions_in_doc(attributed_verses=attributed_verses, query_file=query_file, query_window_len=query_window_len, query_overlap=query_overlap)\n",
    "\n",
    "            for res in results:\n",
    "                final_results[res_id] = res\n",
    "                res_id += 1\n",
    "\n",
    "    final_results_df = pd.DataFrame.from_dict(final_results)\n",
    "    final_results_df = final_results_df.transpose()\n",
    "    \n",
    "    if save:\n",
    "        final_results_df.to_csv(os.path.join(RESULTS_PATH, f'FILTERED_{results_filename}'), encoding='utf-8', quotechar='\"', sep=';')\n",
    "\n",
    "    if return_df:\n",
    "        return final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df = check_results_2(return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: unlike in previous versions, the used translation is ignored, we do not have good data to evaluate this issue well.\n",
    "\n",
    "final_dataframe_column_names = ['verse_id', 'verse_text', 'matched_subverses', 'query_string', 'matched_characters', 'matched_subverses_score', 'exclusives_match',  'multiple_attribution', 'match_probability', 'journal', 'date', 'page_num', 'uuid', 'kramerius_url']\n",
    "\n",
    "\n",
    "def make_full_search_dataframe(results_filename='batch_results.csv', save=True, return_df=False):\n",
    "    \"\"\" This functions converts the preliminary results to structure same as all of the other results (filtered and improved). This is for purely statistical reasons. It only drops duplicates. \"\"\"\n",
    "    # Load results:\n",
    "    results_dataframe = load_results(results_filename)\n",
    "\n",
    "    # Remove duplicate rows from the result dataframe\n",
    "    print('Original size of the results dataframe:', len(results_dataframe))\n",
    "    results_dataframe.drop_duplicates(subset=['verse_id', 'query_file', 'index_query_part'], keep='first', inplace=True)\n",
    "    print('Size of the results dataframe after droping duplicates:', len(results_dataframe))\n",
    "\n",
    "    # Create (empty) final results dataframe:\n",
    "    final_results = {}\n",
    "    res_id = 0\n",
    "\n",
    "    for row_id in results_dataframe.index:\n",
    "        verse_id, query_file, index_query_part, batch_id, ngram_size, query_window_len, query_overlap, ngram_tolerance, edit_distance_tolerance = get_row_data(dataframe=results_dataframe, row_id=row_id)\n",
    "\n",
    "        path_to_json = get_path_to_json_file(query_file)\n",
    "        journal, issue_date, issue_page, issue_uuid, kramerius_url, full_query_string = load_metadata_from_json(json_path=path_to_json)\n",
    "\n",
    "        row_dict = results_dataframe.loc[row_id].to_dict()\n",
    "\n",
    "        row_dict['verse_id'] = verse_id\n",
    "        row_dict['book'] = bip.get_book_id(verse_id)\n",
    "        row_dict['journal'] = journal\n",
    "        row_dict['date'] = issue_date\n",
    "\n",
    "        final_results[res_id] = row_dict\n",
    "        res_id += 1\n",
    "\n",
    "    final_results_df = pd.DataFrame.from_dict(final_results)\n",
    "    final_results_df = final_results_df.transpose()\n",
    "    \n",
    "    if save:\n",
    "        final_results_df.to_csv(os.path.join(RESULTS_PATH, f'UNFILTERED_{results_filename}'), encoding='utf-8', quotechar='\"', sep=';')\n",
    "\n",
    "    if return_df:\n",
    "        return final_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size of the results dataframe: 56741\n",
      "Size of the results dataframe after droping duplicates: 37889\n"
     ]
    }
   ],
   "source": [
    "make_full_search_dataframe('batch_results.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_missing_date(results_filename='batch_results.csv', save=True, return_df=False):\n",
    "    \"\"\" This function repairs wrong date with uuid 334149b0-877c-11e6-8aeb-5ef3fc9ae867 that has as a date 27.-30.06.1927 and not a single date... \"\"\"\n",
    "    results_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=';', encoding='utf-8', index_col=0)\n",
    "    \n",
    "    results_dataframe['date'] = results_dataframe['date'].replace('27.-30.06.1927','30.06.1927')\n",
    "\n",
    "    for row_id in results_dataframe.index:\n",
    "        if results_dataframe.loc[row_id]['uuid'] == '334149b0-877c-11e6-8aeb-5ef3fc9ae867':\n",
    "            print(results_dataframe.loc[row_id]['date'])\n",
    "\n",
    "    if save:\n",
    "        results_dataframe.to_csv(os.path.join(RESULTS_PATH, f'DATECHANGE_{results_filename}'), encoding='utf-8', quotechar='\"', sep=';')\n",
    "\n",
    "    if return_df:\n",
    "        return results_dataframe\n",
    "\n",
    "# TODO: to datum změnit raději přímo  JSON souborech!!! (do publikované finální verze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n",
      "30.06.1927\n"
     ]
    }
   ],
   "source": [
    "repair_missing_date('FILTERED_batch_results_official.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helps for by-hand evaluation\n",
    "Add some description why and what to do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering stop subverses\n",
    "- There are some limits - sometimes only a section is detected due to the use of bad translation... then we are in a risk of loosing a citation (but the general tendency is to have the sure citations and not necessarily all of them...)\n",
    "- There is a chance that this also solves some of the multiple attributions (only the best result may remain)\n",
    "- Usually, when we lost a Biblical reference in this step, it is at worst at the level of allusions, not direct citations (but sure, we have to be careful in the selections)\n",
    "- this simple step (selecting some 326 most \"meaningless\" verses) filtered out 10717 detected citations... well, another 19510 still remained (before: 28674)\n",
    "\n",
    "### OVĚŘIT:\n",
    "- speciálně ověřit výskyt: ['Ty vrchole dokonalosti'] (Ez 28:12), zda to rčení nemá právě oporu v biblickém textu, tedy by bylo často používané, a někdy i biblicky?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_subs(results_filename='batch_results.csv', input_df=False, subverse_len=21, rewrite_original_csv=False, save=True, return_df=False):\n",
    "    \"\"\" This function filters those results that are detected based on solely one subverse that is listed in file evaluation_stop_subverses_{subverse_len}.txt \"\"\"\n",
    "    if input_df is not False:\n",
    "        original_df = input_df\n",
    "    else:\n",
    "        original_df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=';', encoding='utf-8', index_col=0)\n",
    "\n",
    "    print('Length of the original dataframe is:', len(original_df))\n",
    "\n",
    "    with open(os.path.join(ROOT_PATH, f'evaluation_stop_subverses_{subverse_len}.txt'), 'r', encoding='utf-8') as stops_f:\n",
    "        data = stops_f.read()\n",
    "        stop_subs = data.split('\\n')\n",
    "\n",
    "    print('Number of stop subverses to filter:', len(set(stop_subs)))\n",
    "\n",
    "    filtered_df_dict = {}\n",
    "    fil_id = 0  \n",
    "\n",
    "    for row_id in original_df.index:\n",
    "        if original_df.loc[row_id]['matched_subverses'] in stop_subs:\n",
    "            continue\n",
    "        else:\n",
    "            row_as_dict = original_df.loc[row_id].to_dict()            \n",
    "            filtered_df_dict[fil_id] = row_as_dict\n",
    "            fil_id += 1\n",
    "\n",
    "    filtered_df = pd.DataFrame.from_dict(filtered_df_dict)\n",
    "    filtered_df = filtered_df.transpose()\n",
    "\n",
    "    print('Length of the filtered dataframe is:', len(filtered_df))\n",
    "    print('Number of filtered rows:', len(original_df)-len(filtered_df))\n",
    "\n",
    "    if rewrite_original_csv:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if save:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, f'ST_SUBS_{results_filename}'), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if return_df:\n",
    "        return filtered_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the original dataframe is: 28674\n",
      "Number of stop subverses to filter: 327\n",
      "Length of the filtered dataframe is: 17956\n",
      "Number of filtered rows: 10718\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verse_id</th>\n",
       "      <th>verse_text</th>\n",
       "      <th>matched_subverses</th>\n",
       "      <th>query_string</th>\n",
       "      <th>matched_characters</th>\n",
       "      <th>matched_subverses_score</th>\n",
       "      <th>exclusives_match</th>\n",
       "      <th>match_probability</th>\n",
       "      <th>multiple_attribution</th>\n",
       "      <th>journal</th>\n",
       "      <th>date</th>\n",
       "      <th>page_num</th>\n",
       "      <th>uuid</th>\n",
       "      <th>kramerius_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mt 5:38</td>\n",
       "      <td>Slyšeli ste, že řečeno bylo: Oko za oko, a zub...</td>\n",
       "      <td>['Slyšeli ste že řečeno bylo']</td>\n",
       "      <td>Proč neúnavně hlásal, že Starý zákon byl jen p...</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>PRAVDA</td>\n",
       "      <td>Čech</td>\n",
       "      <td>28.07.1927</td>\n",
       "      <td>3</td>\n",
       "      <td>00114990-8782-11e6-8aeb-5ef3fc9ae867</td>\n",
       "      <td>https://kramerius5.nkp.cz/view/uuid:00114990-8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mt 5:43</td>\n",
       "      <td>Slyšeli ste že řečeno bylo: Milovati budeš bli...</td>\n",
       "      <td>['Slyšeli ste že řečeno bylo']</td>\n",
       "      <td>Proč neúnavně hlásal, že Starý zákon byl jen p...</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>PRAVDA</td>\n",
       "      <td>Čech</td>\n",
       "      <td>28.07.1927</td>\n",
       "      <td>3</td>\n",
       "      <td>00114990-8782-11e6-8aeb-5ef3fc9ae867</td>\n",
       "      <td>https://kramerius5.nkp.cz/view/uuid:00114990-8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1J 5:6</td>\n",
       "      <td>Ježíš Kristus jest ten, který přišel skrze vod...</td>\n",
       "      <td>['Ježíš Kristus jest ten']</td>\n",
       "      <td>Týž Fr. W. Förster píše: „To je právě zatwzelo...</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.215909</td>\n",
       "      <td>PRAVDA</td>\n",
       "      <td>Čech</td>\n",
       "      <td>28.07.1927</td>\n",
       "      <td>3</td>\n",
       "      <td>00114990-8782-11e6-8aeb-5ef3fc9ae867</td>\n",
       "      <td>https://kramerius5.nkp.cz/view/uuid:00114990-8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jud 11:21</td>\n",
       "      <td>Splní-li mi to Bůh tvůj, co dobrého mi slibuje...</td>\n",
       "      <td>['co dobrého mi slibuješ']</td>\n",
       "      <td>a tleskal, až mne dlaně pálily, ale cr, Herold...</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.172727</td>\n",
       "      <td>NEPRAVDA</td>\n",
       "      <td>Čech</td>\n",
       "      <td>01.01.1926</td>\n",
       "      <td>3</td>\n",
       "      <td>002dc620-8bff-11e6-8aeb-5ef3fc9ae867</td>\n",
       "      <td>https://kramerius5.nkp.cz/view/uuid:002dc620-8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R 2:12</td>\n",
       "      <td>Ti totiž, kteří zhřešili bez zákona, bez zákon...</td>\n",
       "      <td>['zákonem budou odsouzeni']</td>\n",
       "      <td>Paní Vršíčková nepustivší vaničky z levé ruky,...</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>NEPRAVDA</td>\n",
       "      <td>Čech</td>\n",
       "      <td>01.01.1926</td>\n",
       "      <td>4</td>\n",
       "      <td>002dc620-8bff-11e6-8aeb-5ef3fc9ae867</td>\n",
       "      <td>https://kramerius5.nkp.cz/view/uuid:002dc620-8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    verse_id                                         verse_text  \\\n",
       "0    Mt 5:38  Slyšeli ste, že řečeno bylo: Oko za oko, a zub...   \n",
       "1    Mt 5:43  Slyšeli ste že řečeno bylo: Milovati budeš bli...   \n",
       "2     1J 5:6  Ježíš Kristus jest ten, který přišel skrze vod...   \n",
       "3  Jud 11:21  Splní-li mi to Bůh tvůj, co dobrého mi slibuje...   \n",
       "4     R 2:12  Ti totiž, kteří zhřešili bez zákona, bez zákon...   \n",
       "\n",
       "                matched_subverses  \\\n",
       "0  ['Slyšeli ste že řečeno bylo']   \n",
       "1  ['Slyšeli ste že řečeno bylo']   \n",
       "2      ['Ježíš Kristus jest ten']   \n",
       "3      ['co dobrého mi slibuješ']   \n",
       "4     ['zákonem budou odsouzeni']   \n",
       "\n",
       "                                        query_string matched_characters  \\\n",
       "0  Proč neúnavně hlásal, že Starý zákon byl jen p...           0.961538   \n",
       "1  Proč neúnavně hlásal, že Starý zákon byl jen p...           0.961538   \n",
       "2  Týž Fr. W. Förster píše: „To je právě zatwzelo...           0.863636   \n",
       "3  a tleskal, až mne dlaně pálily, ale cr, Herold...           0.863636   \n",
       "4  Paní Vršíčková nepustivší vaničky z levé ruky,...           0.869565   \n",
       "\n",
       "  matched_subverses_score exclusives_match match_probability  \\\n",
       "0                0.333333              1.0          0.320513   \n",
       "1                0.333333              1.0          0.320513   \n",
       "2                    0.25              1.0          0.215909   \n",
       "3                     0.2              1.0          0.172727   \n",
       "4                0.166667              1.0          0.144928   \n",
       "\n",
       "  multiple_attribution journal        date page_num  \\\n",
       "0               PRAVDA    Čech  28.07.1927        3   \n",
       "1               PRAVDA    Čech  28.07.1927        3   \n",
       "2               PRAVDA    Čech  28.07.1927        3   \n",
       "3             NEPRAVDA    Čech  01.01.1926        3   \n",
       "4             NEPRAVDA    Čech  01.01.1926        4   \n",
       "\n",
       "                                   uuid  \\\n",
       "0  00114990-8782-11e6-8aeb-5ef3fc9ae867   \n",
       "1  00114990-8782-11e6-8aeb-5ef3fc9ae867   \n",
       "2  00114990-8782-11e6-8aeb-5ef3fc9ae867   \n",
       "3  002dc620-8bff-11e6-8aeb-5ef3fc9ae867   \n",
       "4  002dc620-8bff-11e6-8aeb-5ef3fc9ae867   \n",
       "\n",
       "                                       kramerius_url  \n",
       "0  https://kramerius5.nkp.cz/view/uuid:00114990-8...  \n",
       "1  https://kramerius5.nkp.cz/view/uuid:00114990-8...  \n",
       "2  https://kramerius5.nkp.cz/view/uuid:00114990-8...  \n",
       "3  https://kramerius5.nkp.cz/view/uuid:002dc620-8...  \n",
       "4  https://kramerius5.nkp.cz/view/uuid:002dc620-8...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = filter_stop_subs(results_filename='FILTERED_batch_results_all_dates.csv', return_df=True)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple attributions\n",
    "- Multiple attributions do for more than a halfe of the attributions, that is something that can be resolved by a script. Here is a suggested solution.\n",
    "\n",
    "### Steps:\n",
    "- go by rows.\n",
    "- if a row has True for multiple attribution, select all other verses that are attributed to the same passage - uuid, page_num, journal, but also the query_string!\n",
    "- the database will have another column added: \"drop?\"\n",
    "- select the attributed citation with the best match_probability and give False to its \"drop?\"\n",
    "- give True \"drop?\" to the rest --> then just quickly check them manually.\n",
    "- also add a column \"CITATION\" - where the final evaluation wil be applied\n",
    "- hand in hand with this process, evaluate those verses, where we are sure it is a citation\n",
    "    - criteria:\n",
    "        - all subverses are present (when there are 1 to 3 subverses in verse); or at least 50% of subverses are present (4 and more subverses in verse)\n",
    "        - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_multiply_attributed_rows(dataframe:pd.core.frame.DataFrame, row_id):\n",
    "    \"\"\" This finction selects all rows that share same multiple attribution. \"\"\"\n",
    "    uuid = dataframe.loc[row_id]['uuid']\n",
    "    query_string = dataframe.loc[row_id]['query_string']\n",
    "\n",
    "    other_attributions_df = dataframe[dataframe['uuid'] == uuid]\n",
    "    other_attributions_df = other_attributions_df[other_attributions_df['multiple_attribution'] == True]\n",
    "    other_attributions_df = other_attributions_df[other_attributions_df['query_string'] == query_string]\n",
    "\n",
    "    row_ids_to_skip = other_attributions_df.index\n",
    "\n",
    "    return other_attributions_df, row_ids_to_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiple_attributions(subset_dataframe:pd.core.frame.DataFrame):\n",
    "    \"\"\" This function evaluates the DROP value of respective rows. \"\"\"\n",
    "    match_probability_values = []\n",
    "    for row_id in subset_dataframe.index:\n",
    "        match_probability_values.append(subset_dataframe.loc[row_id]['match_probability'])\n",
    "    \n",
    "    best_score = max(match_probability_values)\n",
    "\n",
    "    output_dicts = []\n",
    "\n",
    "    num_of_rows_to_drop = 0\n",
    "    \n",
    "    for i, mpv in enumerate(match_probability_values):\n",
    "        if mpv == best_score:\n",
    "            df_dict = subset_dataframe.loc[subset_dataframe.index[i]].to_dict()\n",
    "            df_dict['drop?'] = False\n",
    "            output_dicts.append(df_dict)\n",
    "        else:\n",
    "            df_dict = subset_dataframe.loc[subset_dataframe.index[i]].to_dict()\n",
    "            df_dict['drop?'] = True\n",
    "            output_dicts.append(df_dict)\n",
    "            num_of_rows_to_drop += 1\n",
    "\n",
    "    return output_dicts, num_of_rows_to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_multiple_attributions(results_filename='ST_SUBS_FILTERED_batch_results.csv', input_df=False, rewrite_original_csv=False, save=True, return_df=False):\n",
    "    \"\"\" This function suggest which of the multiple attribution is the right one. \"\"\"\n",
    "    if input_df is not False:\n",
    "        original_df = input_df\n",
    "    else:\n",
    "        original_df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=';', encoding='utf-8', index_col=0)\n",
    "\n",
    "    output_df_dict = {}\n",
    "    out_idx = 0\n",
    "\n",
    "    num_of_rows_to_drop = 0\n",
    "\n",
    "    rows_to_skip = []\n",
    "\n",
    "    for row_id in original_df.index:\n",
    "        if row_id in rows_to_skip:\n",
    "            continue\n",
    "        else:\n",
    "            if original_df.loc[row_id]['multiple_attribution']:\n",
    "                other_attributions_df, add_to_skip = select_multiply_attributed_rows(dataframe=original_df, row_id=row_id)\n",
    "                rows_to_skip.extend(add_to_skip)\n",
    "                if len(other_attributions_df) == 1:\n",
    "                    row_as_dict = original_df.loc[row_id].to_dict()\n",
    "                    row_as_dict['drop?'] = False\n",
    "                    output_df_dict[out_idx] = row_as_dict\n",
    "                    out_idx += 1\n",
    "                else:\n",
    "                    rows_to_add, rows_to_drop_count = evaluate_multiple_attributions(subset_dataframe=other_attributions_df)\n",
    "                    num_of_rows_to_drop += rows_to_drop_count\n",
    "                    for rta in rows_to_add:\n",
    "                        output_df_dict[out_idx] = rta\n",
    "                        out_idx += 1\n",
    "            else:\n",
    "                row_as_dict = original_df.loc[row_id].to_dict()\n",
    "                row_as_dict['drop?'] = False\n",
    "                output_df_dict[out_idx] = row_as_dict\n",
    "                out_idx += 1\n",
    "\n",
    "    filtered_df = pd.DataFrame.from_dict(output_df_dict)\n",
    "    filtered_df = filtered_df.transpose()\n",
    "\n",
    "    print('Number of rows selected for drop:', num_of_rows_to_drop, 'out of', len(original_df))\n",
    "\n",
    "    if rewrite_original_csv:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if save:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, f'MA_{results_filename}'), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if return_df:\n",
    "        return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolve_multiple_attributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_sure_citations(results_filename='MA_ST_SUBS_FILTERED_batch_results.csv', input_df=False, rewrite_original_csv=False, save=True, return_df=False):\n",
    "    \"\"\" This function marks some of the citations as sure citations while other unsure. \"\"\"\n",
    "    if input_df is not False:\n",
    "        original_df = input_df\n",
    "    else:\n",
    "        original_df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=';', encoding='utf-8', index_col=0)\n",
    "\n",
    "    output_df_dict = {}\n",
    "    out_idx = 0\n",
    "\n",
    "    num_of_sure_citations = 0\n",
    "\n",
    "    for row_id in original_df.index:\n",
    "        row_as_dict = original_df.loc[row_id].to_dict()\n",
    "\n",
    "        num_of_subverses_in_verse = len(bip.split_verse(row_as_dict['verse_text'], tole_len=21))\n",
    "        match_subs_score = row_as_dict['matched_subverses_score']\n",
    "\n",
    "        if num_of_subverses_in_verse <= 2:\n",
    "            if match_subs_score == 1 and row_as_dict['exclusives_match'] == 1:\n",
    "                row_as_dict['CITATION'] = True\n",
    "                num_of_sure_citations += 1\n",
    "            else:\n",
    "                row_as_dict['CITATION'] = False\n",
    "        elif num_of_subverses_in_verse <= 4:\n",
    "            if match_subs_score >= 0.8 and row_as_dict['exclusives_match'] == 1:\n",
    "                row_as_dict['CITATION'] = True\n",
    "                num_of_sure_citations += 1\n",
    "            else:\n",
    "                row_as_dict['CITATION'] = False\n",
    "\n",
    "        else:\n",
    "            if match_subs_score >= 0.5 and row_as_dict['exclusives_match'] == 1:\n",
    "                row_as_dict['CITATION'] = True\n",
    "                num_of_sure_citations += 1\n",
    "            else:\n",
    "                row_as_dict['CITATION'] = False\n",
    "\n",
    "        output_df_dict[out_idx] = row_as_dict\n",
    "        out_idx += 1\n",
    "    \n",
    "    filtered_df = pd.DataFrame.from_dict(output_df_dict)\n",
    "    filtered_df = filtered_df.transpose()\n",
    "\n",
    "    print('Number of rows selected as sure citations:', num_of_sure_citations, 'out of', len(original_df))\n",
    "\n",
    "    if rewrite_original_csv:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if save:\n",
    "        filtered_df.to_csv(os.path.join(RESULTS_PATH, f'FINAL_{results_filename}'), quotechar='\"', sep=';', encoding='utf-8')\n",
    "    \n",
    "    if return_df:\n",
    "        return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows selected as sure citations: 1923 out of 17957\n"
     ]
    }
   ],
   "source": [
    "mark_sure_citations('MA_ST_SUBS_FILTERED_batch_results_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all imporovements at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_filtered_citations(results_filename='FILTERED_batch_results.csv', save=True, return_df=False, save_steps=False):\n",
    "    \"\"\" This function runs all of the above prepared functions that together evaluate the preliminary discoverd citations. \"\"\"\n",
    "    stop_subs_df = filter_stop_subs(results_filename=results_filename, save=save_steps, return_df=True)\n",
    "    multiple_attrs_df = resolve_multiple_attributions(results_filename=f'ST_SUBS_{results_filename}', input_df=stop_subs_df, save=save_steps, return_df=True)\n",
    "    final_df = mark_sure_citations(results_filename=results_filename, input_df=multiple_attrs_df, save=save, return_df=return_df)\n",
    "    \n",
    "    if return_df:\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the original dataframe is: 28666\n",
      "Number of stop subverses to filter: 327\n",
      "Length of the filtered dataframe is: 17948\n",
      "Number of filtered rows: 10718\n",
      "Number of rows selected for drop: 4658 out of 17948\n",
      "Number of rows selected as sure citations: 926 out of 17948\n"
     ]
    }
   ],
   "source": [
    "evaluate_filtered_citations('FILTERED_batch_results_full.csv', save_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_only_sure_citations(results_filename='FINAL_FILTERED_batch_results.csv', save=True, return_df=False):\n",
    "    \"\"\" This function filters only the citations that are marked as \"sure\" citations, and it also drops those that have the \"drop\" mark. \"\"\"\n",
    "    original_df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=';', encoding='utf-8', index_col=0)\n",
    "\n",
    "    output_df_dict = {}\n",
    "    out_idx = 0\n",
    "    \n",
    "    for row_id in original_df.index:\n",
    "        if original_df.loc[row_id]['CITATION'] == True and original_df.loc[row_id]['drop?'] == False:\n",
    "            output_df_dict[out_idx] = original_df.loc[row_id].to_dict()\n",
    "            out_idx += 1\n",
    "\n",
    "    output_df = pd.DataFrame.from_dict(output_df_dict)\n",
    "    output_df = output_df.transpose()\n",
    "\n",
    "    if save:\n",
    "        output_df.to_csv(os.path.join(RESULTS_PATH, f'CITATIONS_{results_filename}'), quotechar='\"', sep=',', encoding='utf-8')\n",
    "\n",
    "    if return_df:\n",
    "        return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_only_sure_citations(results_filename='FINAL_FILTERED_batch_results_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_date_to_year(results_filename='CITATIONS_FINAL_FILTERED_batch_results.csv'):\n",
    "    \"\"\" This function changes the dates to years only. \"\"\"\n",
    "    original_df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=',', encoding='utf-8', index_col=0)\n",
    "\n",
    "    output_df_dict = {}\n",
    "    out_idx = 0\n",
    "    \n",
    "    for row_id in original_df.index:\n",
    "        date = original_df.loc[row_id]['date']\n",
    "        date_parts = date.split('.')\n",
    "\n",
    "        row_dict = original_df.loc[row_id].to_dict()\n",
    "        \n",
    "        row_dict['date'] = date_parts[2]\n",
    "        \n",
    "        output_df_dict[out_idx] = row_dict\n",
    "        out_idx += 1\n",
    "\n",
    "    output_df = pd.DataFrame.from_dict(output_df_dict)\n",
    "    output_df = output_df.transpose()\n",
    "\n",
    "    output_df.to_csv(os.path.join(RESULTS_PATH, f'YEAR_{results_filename}'), quotechar='\"', sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_date_to_year(results_filename='CITATIONS_FINAL_FILTERED_batch_results_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table_by_years_for_verse(results_filename='YEAR_CITATIONS_FINAL_FILTERED_batch_results_full.csv'):\n",
    "    \"\"\" This function makes a csv that includes only the verse ids and their number in individual years. \"\"\"\n",
    "    original_df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=',', encoding='utf-8', index_col=0)\n",
    "\n",
    "    all_cited_verses = list(set(original_df['verse_id'].values.tolist()))\n",
    "\n",
    "    years = [1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939]\n",
    "\n",
    "    output_dict = {}\n",
    "    out_idx = 0\n",
    "\n",
    "    for verse_id in all_cited_verses:\n",
    "        verse_df = original_df[original_df['verse_id'] == verse_id]\n",
    "        verse_dict = {}\n",
    "        verse_dict['verse_id'] = verse_id\n",
    "        for year in years:\n",
    "            year_df = verse_df[verse_df['date'] == year]\n",
    "            verse_dict[year] = len(year_df)\n",
    "        \n",
    "        output_dict[out_idx] = verse_dict\n",
    "        out_idx += 1\n",
    "\n",
    "    output_df = pd.DataFrame.from_dict(output_dict)\n",
    "    output_df = output_df.transpose()\n",
    "\n",
    "    output_df.to_csv(os.path.join(RESULTS_PATH, 'citations_by_year.csv'), quotechar='\"', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table_by_years_for_verse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_citations(results_filename='YEAR_CITATIONS_FINAL_FILTERED_batch_results_full.csv'):\n",
    "    \"\"\" This function makes a csv that includes only the verse ids and their number in individual years. \"\"\"\n",
    "    original_df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=',', encoding='utf-8', index_col=0)\n",
    "\n",
    "    all_cited_verses = list(set(original_df['verse_id'].values.tolist()))\n",
    "\n",
    "    output_dict = {}\n",
    "    out_idx = 0\n",
    "\n",
    "    for verse_id in all_cited_verses:\n",
    "        verse_df = original_df[original_df['verse_id'] == verse_id]\n",
    "        verse_dict = {}\n",
    "        verse_dict['verse_id'] = verse_id\n",
    "        verse_dict['count'] = len(verse_df)\n",
    "        verse_dict['verse_text'] = verse_df.iloc[0]['verse_text']\n",
    "\n",
    "        \n",
    "        output_dict[out_idx] = verse_dict\n",
    "        out_idx += 1\n",
    "\n",
    "    output_df = pd.DataFrame.from_dict(output_dict)\n",
    "    output_df = output_df.transpose()\n",
    "\n",
    "    output_df.to_csv(os.path.join(RESULTS_PATH, 'aggregated_citations.csv'), quotechar='\"', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_citations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_Gn_1_1(results_filename):\n",
    "    \"\"\" This function is used to filter out Gn 1:1 in BSV translation - because there has been a mess up and the text is was wrong there 'První Knihy Mojžíšovy' \"\"\"\n",
    "    df = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=';', encoding='utf-8', index_col=0)\n",
    "\n",
    "    print(len(df))\n",
    "\n",
    "    output_dict = {}\n",
    "    out_idx = 0\n",
    "    skipped = 0\n",
    "    for row_id in df.index:\n",
    "        row_dict = df.loc[row_id].to_dict()\n",
    "        if row_dict['verse_text'] == 'První Knihy Mojžíšovy':\n",
    "            skipped += 1\n",
    "            continue\n",
    "        else:\n",
    "            output_dict[out_idx] = row_dict\n",
    "            out_idx += 1\n",
    "\n",
    "    print(skipped)\n",
    "\n",
    "    output_df = pd.DataFrame.from_dict(output_dict)\n",
    "    output_df = output_df.transpose()\n",
    "\n",
    "    print(len(output_df))\n",
    "\n",
    "    print((len(output_df)+skipped), (len(df)))\n",
    "\n",
    "    output_df.to_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28674\n",
      "8\n",
      "28666\n",
      "28674 28674\n"
     ]
    }
   ],
   "source": [
    "filter_Gn_1_1('FILTERED_batch_results_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Same\" verses\n",
    "- this part connects those verses that have the same text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_verses = {\n",
    "    'L 11:3/Mt 6:11': ['L 11:3', 'Mt 6:11'],\n",
    "    'Mk 13:31/Mt 24:35/L 21:33': ['Mk 13:31', 'Mt 24:35', 'L 21:33'],\n",
    "    'Ex 20:16/Dt 5:20': ['Ex 20:16', 'Dt 5:20'],\n",
    "    '2K 1:2/Fp 1:2/2Te 1:2/1K 1:3/Ef 1:2/Ga 1:3': ['2K 1:2', 'Fp 1:2', '2Te 1:2', '1K 1:3', 'Ef 1:2', 'Ga 1:3'],\n",
    "    'Mt 11:15/Mt 13:9': ['Mt 11:15', 'Mt 13:9']\n",
    "    \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8427d73f02270cccde5d7b657eb9efbd549974ced0a163dd64bdafb1c87004cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
