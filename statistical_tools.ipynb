{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STatistical tools for BiblicalIntertextuality\n",
    "\n",
    "This jupyter notebook contains functions that allows you to process statistics out of the data corpus (both journals and the Bible) and out of the results file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import biblical_intertextuality_package as bip\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from biblical_intertextuality_package import split_verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.getcwd()\n",
    "\n",
    "BIBLES_PATH = os.path.join(ROOT_PATH, 'Bible_files')\n",
    "DATASETS_PATH = os.path.join(ROOT_PATH, 'datasets')\n",
    "DICTS_PATH = os.path.join(ROOT_PATH, 'dictionaries')\n",
    "CORPUS_PATH = os.path.join(ROOT_PATH, 'corpuses')\n",
    "ALL_JSONS_PATH = os.path.join(ROOT_PATH, 'query_jsons')\n",
    "\n",
    "JOURNAL_FULLDATA_PATH = os.path.join(ROOT_PATH, 'journals_fulldata.joblib')\n",
    "\n",
    "# RESULTS_PATH = os.path.join(ROOT_PATH, 'results')\n",
    "RESULTS_PATH = os.path.join(ROOT_PATH, 'PUBLIC_RESULTS')\n",
    "BATCHES_FILE_PATH = os.path.join(ROOT_PATH, 'batches.csv')\n",
    "BATCH_RESULTS_FILE_PATH = os.path.join(RESULTS_PATH, 'batch_results.csv')\n",
    "\n",
    "STOP_WORDS_PATH = os.path.join(ROOT_PATH, 'stop_words.txt')\n",
    "STOP_SUBVERSES_PATH = os.path.join(ROOT_PATH, 'stop_subverses_21.txt')\n",
    "EXCLUSIVES_PATH = os.path.join(ROOT_PATH, 'exclusives.txt')\n",
    "\n",
    "EVALUATION_STOP_SUBVERSES_PATH = os.path.join(ROOT_PATH, 'evaluation_stop_subverses_21.txt')\n",
    "FULL_HIT_NEEDED_SUBS_PATH = os.path.join(ROOT_PATH, '100_hit_needed_subs_21.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bible statistics\n",
    "Following functions return statistics about Bible files as stored in Bible_files directory. Be aware that we as the developers have an additional translation at our disposal ('Bible Svatováclavská') so your statistics will be probably different.\n",
    "\n",
    "- number of translations at our disposal: 5\n",
    "    - one is both Old and New Testament (\"Bible Kralická\")\n",
    "    - translation of Jan Hejčl: Old Testament + deuterocanonical books\n",
    "    - \"Bible of the Saint Venceslas\": mix of Old and New Testament, not complete (even some individual verses are missing)\n",
    "    - translation of Ladislav Sýkora: New Testament\n",
    "    - translation of František Žilka: New Testament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_names = {'BKR': 'Bible Kralická', 'BSV': 'Bible Svatováclavská', 'HEJCL': 'Jan Hejčl', 'SYK': 'Ladislav Sýkora', 'ZP': 'František Žilka'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_filename_data(bible_file_name:str):\n",
    "    bible_file_name = bible_file_name[:-4]\n",
    "    filename_parts = bible_file_name.split('_')\n",
    "\n",
    "    translation = filename_parts[1]\n",
    "    book_name = filename_parts[2]\n",
    "\n",
    "    return translation, book_name\n",
    "\n",
    "\n",
    "def analise_the_bible():\n",
    "    \"\"\"\n",
    "    This function analyses all data in the Bible_files directory. You can change split_verse settings if you have created your dataset with different settings.\n",
    "    \n",
    "    It prints following statistics:\n",
    "    - number of translations and their abbreviated names\n",
    "    - number of books (in aggregation and for each translation separately)\n",
    "    - number of verses (by verse_id, all in aggregation, and for each translation separately)\n",
    "    - number of subverses (in aggregation and for each translation separately)\n",
    "\n",
    "    And it saves csv file with statistics of books, verses and subverses for each translation (into RESULTS_PATH (by defaut now PUBLIC_RESULTS directory) --> statistics).\n",
    "    \"\"\"\n",
    "    all_bible_files = os.listdir(BIBLES_PATH)\n",
    "\n",
    "    translations = []\n",
    "    \n",
    "    books_names = []\n",
    "    books_aggregated = 0\n",
    "    books_per_trsl = defaultdict(int)\n",
    "\n",
    "    verse_ids = []\n",
    "    verses_aggregated = 0\n",
    "    verses_per_trsl = defaultdict(int)\n",
    "\n",
    "    subverses_aggregated = 0\n",
    "    subverses_per_trsl = defaultdict(int)\n",
    "\n",
    "    for bible_file in all_bible_files:\n",
    "        translation, book_name = get_book_filename_data(bible_file)\n",
    "        \n",
    "        translations.append(translation)\n",
    "\n",
    "        books_names.append(book_name)\n",
    "        books_aggregated += 1\n",
    "        books_per_trsl[translation] += 1\n",
    "\n",
    "        with open(os.path.join(BIBLES_PATH, bible_file), 'r', encoding='utf-8') as b_file:\n",
    "            data = b_file.read()\n",
    "            verses = eval(data)\n",
    "\n",
    "            for verse_id in verses:\n",
    "                verse_ids.append(verse_id)\n",
    "                verses_aggregated += 1\n",
    "                verses_per_trsl[translation] += 1\n",
    "\n",
    "                subverses = split_verse(verses[verse_id])\n",
    "\n",
    "                subverses_aggregated += len(subverses)\n",
    "                subverses_per_trsl[translation] += len(subverses)\n",
    "\n",
    "    translations = list(set(translations))\n",
    "\n",
    "    print(translations)\n",
    "    print('Numebr of translations:', len(translations))\n",
    "\n",
    "    print()\n",
    "\n",
    "    books_names = list(set(books_names))\n",
    "    print('Number of available books:', len(books_names))\n",
    "    print('Number of books accross translations:', books_aggregated)\n",
    "    for trsl in books_per_trsl:\n",
    "        print('Number of books in', trsl, 'is', books_per_trsl[trsl])\n",
    "\n",
    "    print()\n",
    "\n",
    "    verse_ids = list(set(verse_ids))\n",
    "    print('Number of available verses:', len(verse_ids))\n",
    "    print('Number of verses accross translations:', verses_aggregated)\n",
    "    for trsl in verses_per_trsl:\n",
    "        print('Number of verses in', trsl, 'is', verses_per_trsl[trsl])\n",
    "\n",
    "    print()\n",
    "    \n",
    "    print('Total number of subverses:', subverses_aggregated)\n",
    "    for trsl in subverses_per_trsl:\n",
    "        print('Number of subverses in', trsl, 'is', subverses_per_trsl[trsl])\n",
    "\n",
    "    df_dict = {'translation': [], 'books': [], 'verses': [], 'subverses': []}\n",
    "    for trsl in translations:\n",
    "        df_dict['translation'].append(trsl)\n",
    "        df_dict['books'].append(books_per_trsl[trsl])\n",
    "        df_dict['verses'].append(verses_per_trsl[trsl])\n",
    "        df_dict['subverses'].append(subverses_per_trsl[trsl])\n",
    "\n",
    "    \n",
    "    bible_stats_df = pd.DataFrame(df_dict)\n",
    "    bible_stats_df.to_csv(os.path.join(RESULTS_PATH, 'statistics', 'bible_stats.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BKR', 'SYK', 'HEJCL', 'BSV', 'ZP']\n",
      "Numebr of translations: 5\n",
      "\n",
      "Number of available books: 75\n",
      "Number of books accross translations: 201\n",
      "Number of books in BKR is 65\n",
      "Number of books in BSV is 37\n",
      "Number of books in HEJCL is 47\n",
      "Number of books in SYK is 26\n",
      "Number of books in ZP is 26\n",
      "\n",
      "Number of available verses: 38190\n",
      "Number of verses accross translations: 89080\n",
      "Number of verses in BKR is 30767\n",
      "Number of verses in BSV is 15326\n",
      "Number of verses in HEJCL is 27896\n",
      "Number of verses in SYK is 7541\n",
      "Number of verses in ZP is 7550\n",
      "\n",
      "Total number of subverses: 327972\n",
      "Number of subverses in BKR is 119388\n",
      "Number of subverses in BSV is 55660\n",
      "Number of subverses in HEJCL is 103523\n",
      "Number of subverses in SYK is 24786\n",
      "Number of subverses in ZP is 24615\n"
     ]
    }
   ],
   "source": [
    "analise_the_bible()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journals data statistics\n",
    "\n",
    "The following section contains functions that facilitace journals statistics. Since the sournals data are not availabe within the GitHub repository, these functions are pretty much useless for you, unles you wish to analyse your own datasets.\n",
    "\n",
    "- NOTE: this process also revelas some mistakes in the dataset. For example Čech no. 26 from 1935 has no marked date in our dataset and [online](https://kramerius5.nkp.cz/view/uuid:334149b0-877c-11e6-8aeb-5ef3fc9ae867) is marek as issued in 1927. In general, such mistakes are ignored because statistically insignificant, but the researches should be avere of these problems (and may repair such mistakes in their datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_journals():\n",
    "    \"\"\"\n",
    "    This function analyses the available journal files (based on the 'journals_fulldata.joblib' file)\n",
    "\n",
    "    It prints following data:\n",
    "    - number of analysed journals\n",
    "    - number of years that each journal covers\n",
    "    - number of issues per journal\n",
    "    - number of pages per journal\n",
    "    - number of charaters per journal\n",
    "    - average characters per page per journal\n",
    "    \"\"\"\n",
    "    # NOTE: because we are following only years 1925-1939, we are ignoring those that are outside of this scope... '1937-1938' is added because it fits the profile, too...\n",
    "    years_to_consider = ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '1937-1938']\n",
    "\n",
    "    journals_fulldata_dict = joblib.load(JOURNAL_FULLDATA_PATH)\n",
    "\n",
    "    journals = []\n",
    "    years_per_yournal = defaultdict(list)\n",
    "    issues_per_journal = defaultdict(list)\n",
    "    pages_per_journal = defaultdict(int)\n",
    "    characters_per_journal = defaultdict(int)\n",
    "\n",
    "    uuids_out_of_the_scope = []\n",
    "    years_out_of_scope = []\n",
    "    uuids_without_date = []\n",
    "    \n",
    "    for uuid_file in journals_fulldata_dict:\n",
    "        uuid_file_data = journals_fulldata_dict[uuid_file]\n",
    "        \n",
    "        journal = uuid_file_data['journal']\n",
    "        issue_date = uuid_file_data['issue_date']\n",
    "        \n",
    "        if issue_date == '':\n",
    "            uuids_without_date.append(uuid_file)\n",
    "            continue\n",
    "        else:\n",
    "            date_parts = issue_date.split('.')\n",
    "            issue_year = date_parts[-1]\n",
    "\n",
    "            if issue_year in years_to_consider:\n",
    "                if issue_year not in years_per_yournal[journal]:\n",
    "                    if issue_year == '1937-1938':\n",
    "                        continue\n",
    "                    else:\n",
    "                        years_per_yournal[journal].append(issue_year)\n",
    "        \n",
    "                if journal not in journals:\n",
    "                    journals.append(journal)                \n",
    "\n",
    "                uuid = uuid_file_data['issue_uuid']\n",
    "                if uuid not in issues_per_journal[journal]:\n",
    "                    issues_per_journal[journal].append(uuid)\n",
    "\n",
    "                pages_per_journal[journal] += 1\n",
    "\n",
    "                characters_per_journal[journal] += len(uuid_file_data['text'])\n",
    "            \n",
    "            else:\n",
    "                uuids_out_of_the_scope.append(uuid_file)\n",
    "                years_out_of_scope.append(issue_year)\n",
    "\n",
    "    print('Number of analysed journals:', len(journals))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for journal in journals:\n",
    "        years_per_yournal[journal].sort()\n",
    "        print(journal)\n",
    "        print(f'\\tYears in journal ({len(years_per_yournal[journal])}):', years_per_yournal[journal])\n",
    "        print('\\tIssues in journal:', len(issues_per_journal[journal]))\n",
    "        print('\\tPages in journal:', pages_per_journal[journal])\n",
    "        print('\\tCharacter in journal:', characters_per_journal[journal])\n",
    "        print('\\tAverage characters per page in journal:', characters_per_journal[journal]/pages_per_journal[journal])\n",
    "\n",
    "    print()\n",
    "\n",
    "    print('Number of files that do not fit the time range:', len(uuids_out_of_the_scope), set(years_out_of_scope))\n",
    "    print('Number of files that do not have date in the metadata:', len(uuids_without_date))\n",
    "\n",
    "    df_dict = {'journal': [], 'years': [], 'issues': [], 'pages': [], 'standard pages': [], 'characters': []}\n",
    "    for journal in journals:\n",
    "        df_dict['journal'].append(journal)\n",
    "        df_dict['years'].append(len(years_per_yournal[journal]))\n",
    "        df_dict['issues'].append(len(issues_per_journal[journal]))\n",
    "        df_dict['pages'].append(pages_per_journal[journal])\n",
    "        df_dict['standard pages'].append(characters_per_journal[journal]/1800)\n",
    "        df_dict['characters'].append(characters_per_journal[journal])\n",
    "\n",
    "    \n",
    "    journals_stats_df = pd.DataFrame(df_dict)\n",
    "    journals_stats_df.to_csv(os.path.join(RESULTS_PATH, 'statistics', 'journals_stats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of analysed journals: 11\n",
      "\n",
      "Čech\n",
      "\tYears in journal (10): ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934']\n",
      "\tIssues in journal: 2368\n",
      "\tPages in journal: 15678\n",
      "\tCharacter in journal: 145508697\n",
      "\tAverage characters per page in journal: 9281.075200918485\n",
      "Československý zemědělec\n",
      "\tYears in journal (15): ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939']\n",
      "\tIssues in journal: 753\n",
      "\tPages in journal: 9278\n",
      "\tCharacter in journal: 91147459\n",
      "\tAverage characters per page in journal: 9824.04171157577\n",
      "Český učitel\n",
      "\tYears in journal (15): ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939']\n",
      "\tIssues in journal: 534\n",
      "\tPages in journal: 11580\n",
      "\tCharacter in journal: 57198594\n",
      "\tAverage characters per page in journal: 4939.429533678756\n",
      "Moravský hospodář\n",
      "\tYears in journal (15): ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939']\n",
      "\tIssues in journal: 350\n",
      "\tPages in journal: 5683\n",
      "\tCharacter in journal: 27342938\n",
      "\tAverage characters per page in journal: 4811.356325884216\n",
      "Moravský večerník\n",
      "\tYears in journal (15): ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939']\n",
      "\tIssues in journal: 4551\n",
      "\tPages in journal: 27907\n",
      "\tCharacter in journal: 308771689\n",
      "\tAverage characters per page in journal: 11064.309635575304\n",
      "Polední list\n",
      "\tYears in journal (4): ['1930', '1931', '1932', '1933']\n",
      "\tIssues in journal: 1632\n",
      "\tPages in journal: 10979\n",
      "\tCharacter in journal: 166354770\n",
      "\tAverage characters per page in journal: 15152.08762182348\n",
      "Posel záhrobní\n",
      "\tYears in journal (15): ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939']\n",
      "\tIssues in journal: 150\n",
      "\tPages in journal: 2828\n",
      "\tCharacter in journal: 7106018\n",
      "\tAverage characters per page in journal: 2512.7362093352194\n",
      "Přítomnost\n",
      "\tYears in journal (15): ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939']\n",
      "\tIssues in journal: 775\n",
      "\tPages in journal: 14996\n",
      "\tCharacter in journal: 92878244\n",
      "\tAverage characters per page in journal: 6193.5345425446785\n",
      "Studentský časopis\n",
      "\tYears in journal (15): ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939']\n",
      "\tIssues in journal: 145\n",
      "\tPages in journal: 5410\n",
      "\tCharacter in journal: 28280046\n",
      "\tAverage characters per page in journal: 5227.365249537893\n",
      "Venkov\n",
      "\tYears in journal (15): ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939']\n",
      "\tIssues in journal: 4589\n",
      "\tPages in journal: 60994\n",
      "\tCharacter in journal: 1102530937\n",
      "\tAverage characters per page in journal: 18076.05562842247\n",
      "Věstník katolického duchovenstva\n",
      "\tYears in journal (15): ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939']\n",
      "\tIssues in journal: 175\n",
      "\tPages in journal: 1930\n",
      "\tCharacter in journal: 10213262\n",
      "\tAverage characters per page in journal: 5291.845595854922\n",
      "\n",
      "Number of files that do not fit the time range: 458 {'1924', '2016'}\n",
      "Number of files that do not have date in the metadata: 0\n"
     ]
    }
   ],
   "source": [
    "analyse_journals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime statistics\n",
    "Statistics of runtimes - based on batches.csv file (the runtimes are always averages per batch, not per single documents!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_runtimes():\n",
    "    \"\"\"\n",
    "    This function analyses runtimes from the batches.csv file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join(ROOT_PATH, 'batches.csv'), quotechar='\"', delimiter=',', encoding='utf-8')\n",
    "\n",
    "    journals_full_names = ['Moravský hospodář', 'Polední list', 'Moravský večerník', 'Věstník katolického duchovenstva', 'Přítomnost', 'Venkov', 'Studentský časopis', 'Československý zemědělec', 'Český učitel', 'Čech', 'Posel záhrobní']\n",
    "    journals_filenames = ['moravsky_hospodar', 'poledni_list', 'moravsky_vecernik', 'vestnik_katolickeho_duchovenstva', 'pritomnost', 'venkov', 'studentsky_casopis', 'ceskoslovensky_zemedelec', 'cesky_ucitel', 'cech', 'posel_zahrobni']\n",
    "\n",
    "    all_times = []\n",
    "    results = {}\n",
    "    \n",
    "    for i, journal in enumerate(journals_filenames):\n",
    "        subset_dataframe = df[df['journal'] == journal]\n",
    "        journal_runtimes = defaultdict(int)\n",
    "        for row_id in subset_dataframe.index:\n",
    "            runtime = subset_dataframe.loc[row_id]['runtime']\n",
    "            if runtime >= 15:\n",
    "                # ignoring wierdly long runtimes (probably due to computer in sleep mode...)\n",
    "                continue\n",
    "            else:\n",
    "                journal_runtimes[round(runtime, 1)] += 1\n",
    "                all_times.append(round(runtime, 1))\n",
    "\n",
    "        print(journals_full_names[i], journal_runtimes)\n",
    "        results[journals_full_names[i]] = journal_runtimes\n",
    "    \n",
    "    all_times_full = [round(i, 1) for i in np.arange(0, 10, 0.1)]\n",
    "\n",
    "    output_dict = {}\n",
    "    out_idx = 0\n",
    "    \n",
    "    for res in results:\n",
    "        journal_data = {}\n",
    "        journal_data['journal'] = res\n",
    "        for timestamp in all_times_full:\n",
    "            try:\n",
    "                journal_data[timestamp] = results[res][timestamp]\n",
    "            except IndexError:\n",
    "                journal_data[timestamp] = 0\n",
    "        \n",
    "        output_dict[out_idx] = journal_data\n",
    "        out_idx += 1\n",
    "\n",
    "    output_df = pd.DataFrame.from_dict(output_dict)\n",
    "    output_df = output_df.transpose()\n",
    "\n",
    "    output_df.to_csv(os.path.join(RESULTS_PATH, 'statistics', 'runtimes_stats.csv'), quotechar='\"', sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moravský hospodář defaultdict(<class 'int'>, {0.8: 1880, 0.7: 1880, 0.9: 520, 0.6: 1200, 0.5: 160, 1.1: 40, 1.8: 3})\n",
      "Polední list defaultdict(<class 'int'>, {2.4: 3480, 2.5: 1960, 2.6: 2360, 2.2: 640, 2.3: 1600, 2.7: 360, 2.8: 280, 2.9: 40, 2.1: 240, 1.0: 19})\n",
      "Moravský večerník defaultdict(<class 'int'>, {1.8: 5480, 2.1: 3600, 2.0: 4480, 1.9: 4280, 1.6: 2160, 1.7: 3000, 2.2: 1960, 2.3: 720, 1.4: 160, 1.5: 1000, 2.4: 673, 1.3: 80, 2.6: 120, 2.7: 40, 2.5: 160})\n",
      "Věstník katolického duchovenstva defaultdict(<class 'int'>, {1.9: 40, 1.0: 400, 1.3: 120, 1.2: 480, 1.1: 680, 0.9: 200, 1.4: 40, 0.0: 2})\n",
      "Přítomnost defaultdict(<class 'int'>, {0.9: 40, 1.5: 4080, 1.3: 1800, 1.4: 4396, 1.6: 2600, 1.2: 880, 1.7: 640, 1.8: 320, 2.1: 40, 1.1: 80, 2.8: 40, 1.9: 40, 2.0: 40})\n",
      "Venkov defaultdict(<class 'int'>, {2.4: 760, 3.6: 920, 3.2: 7480, 3.4: 2880, 2.9: 7760, 2.8: 7240, 3.5: 1440, 3.1: 8960, 3.3: 4480, 3.8: 320, 3.0: 10120, 2.7: 3560, 2.6: 2080, 2.5: 800, 2.2: 400, 2.3: 240, 3.7: 280, 4.3: 120, 3.9: 80, 4.0: 40, 4.1: 120, 4.4: 40, 4.2: 40, 1.7: 80, 1.3: 80, 2.1: 160, 2.0: 120, 1.9: 114, 1.6: 120, 1.8: 40, 1.1: 40})\n",
      "Studentský časopis defaultdict(<class 'int'>, {1.4: 40, 1.1: 800, 1.0: 2120, 1.2: 240, 0.9: 1680, 0.8: 640, 2.4: 26})\n",
      "Československý zemědělec defaultdict(<class 'int'>, {2.0: 680, 2.8: 160, 2.9: 160, 2.4: 320, 3.1: 40, 1.7: 680, 2.5: 160, 2.6: 240, 2.3: 360, 1.9: 560, 3.0: 80, 2.7: 160, 2.2: 560, 2.1: 440, 3.3: 40, 4.3: 40, 4.9: 40, 5.8: 40, 4.4: 40, 1.8: 1040, 1.4: 920, 1.5: 1240, 1.6: 1000, 1.3: 200, 1.2: 78})\n",
      "Český učitel defaultdict(<class 'int'>, {1.2: 280, 1.0: 4120, 1.1: 760, 0.8: 2224, 0.9: 4120, 0.6: 200, 0.7: 160})\n",
      "Čech defaultdict(<class 'int'>, {1.8: 6240, 1.7: 2240, 1.9: 4320, 2.2: 120, 2.0: 1718, 2.1: 560, 1.5: 80, 1.6: 400})\n",
      "Posel záhrobní defaultdict(<class 'int'>, {1.0: 40, 0.8: 160, 0.6: 1400, 0.5: 320, 0.7: 840, 0.9: 72})\n"
     ]
    }
   ],
   "source": [
    "analyse_runtimes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results statistics\n",
    "\n",
    "The following section contains functions that facilitate results stiatistics\n",
    "\n",
    "### Overall statistics for journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_journals = ['Moravský hospodář', 'Polední list', 'Moravský večerník', 'Věstník katolického duchovenstva', 'Přítomnost', 'Venkov', 'Studentský časopis', 'Československý zemědělec', 'Český učitel', 'Čech', 'Posel záhrobní']\n",
    "\n",
    "def ciation_counts_per_journal(results_filename:str, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "\n",
    "    print('Total number of citations:', len(citations_dataframe))\n",
    "    \n",
    "    citations = {}\n",
    "    \n",
    "    for journal in all_journals:\n",
    "        journal_citations_df = citations_dataframe[citations_dataframe['journal'] == journal]\n",
    "        print(journal, len(journal_citations_df))\n",
    "        citations[journal] = len(journal_citations_df)\n",
    "\n",
    "    return citations\n",
    "\n",
    "\n",
    "def ciation_counts_per_journal_not_drop(results_filename:str, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "\n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "    \n",
    "    print('Total number of citations:', len(citations_dataframe))\n",
    "    \n",
    "    citations = {}\n",
    "    \n",
    "    for journal in all_journals:\n",
    "        journal_citations_df = citations_dataframe[citations_dataframe['journal'] == journal]\n",
    "        print(journal, len(journal_citations_df))\n",
    "        citations[journal] = len(journal_citations_df)\n",
    "\n",
    "    return citations\n",
    "\n",
    "\n",
    "def sure_ciations_counts_per_journal(results_filename:str, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "\n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['CITATION'] == True]\n",
    "    \n",
    "    print('Total number of citations:', len(citations_dataframe))\n",
    "    \n",
    "    citations = {}\n",
    "    \n",
    "    for journal in all_journals:\n",
    "        journal_citations_df = citations_dataframe[citations_dataframe['journal'] == journal]\n",
    "        print(journal, len(journal_citations_df))\n",
    "        citations[journal] = len(journal_citations_df) \n",
    "    \n",
    "    return citations\n",
    "\n",
    "\n",
    "def get_overall_result_stats():\n",
    "    print('Filtered citations:')\n",
    "    filtered_citations = ciation_counts_per_journal('FILTERED_UNFILTERED_batch_results.csv')\n",
    "    print('\\nStop subs:')\n",
    "    after_stop_subs = ciation_counts_per_journal('ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "    print('\\nMultiple attrs:')\n",
    "    multiple_attrs = ciation_counts_per_journal_not_drop('MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "    print('\\n\"Sure\" citations:')\n",
    "    sure_cites = sure_ciations_counts_per_journal('FINAL_MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "\n",
    "    out_df_dict = {'journal': [], 'initial results': [], 'filtered stop-subverses': [], 'filtered multiple attributions': [], '\"sure\" citations': []}\n",
    "\n",
    "    for journal in all_journals:\n",
    "        out_df_dict['journal'].append(journal)\n",
    "        out_df_dict['initial results'].append(filtered_citations[journal])\n",
    "        out_df_dict['filtered stop-subverses'].append(after_stop_subs[journal])\n",
    "        out_df_dict['filtered multiple attributions'].append(multiple_attrs[journal])\n",
    "        out_df_dict['\"sure\" citations'].append(sure_cites[journal])\n",
    "\n",
    "    stats_df = pd.DataFrame(out_df_dict)\n",
    "    stats_df.to_csv(os.path.join(RESULTS_PATH, 'statistics', 'journals_results_stats.csv'))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered citations:\n",
      "Total number of citations: 38004\n",
      "Moravský hospodář 285\n",
      "Polední list 2333\n",
      "Moravský večerník 4406\n",
      "Věstník katolického duchovenstva 2008\n",
      "Přítomnost 2204\n",
      "Venkov 14299\n",
      "Studentský časopis 505\n",
      "Československý zemědělec 1151\n",
      "Český učitel 1109\n",
      "Čech 8264\n",
      "Posel záhrobní 1440\n",
      "\n",
      "Stop subs:\n",
      "Total number of citations: 12858\n",
      "Moravský hospodář 11\n",
      "Polední list 262\n",
      "Moravský večerník 691\n",
      "Věstník katolického duchovenstva 1536\n",
      "Přítomnost 621\n",
      "Venkov 2577\n",
      "Studentský časopis 184\n",
      "Československý zemědělec 219\n",
      "Český učitel 405\n",
      "Čech 5298\n",
      "Posel záhrobní 1054\n",
      "\n",
      "Multiple attrs:\n",
      "Total number of citations: 8121\n",
      "Moravský hospodář 10\n",
      "Polední list 199\n",
      "Moravský večerník 497\n",
      "Věstník katolického duchovenstva 790\n",
      "Přítomnost 434\n",
      "Venkov 1975\n",
      "Studentský časopis 144\n",
      "Československý zemědělec 151\n",
      "Český učitel 259\n",
      "Čech 3175\n",
      "Posel záhrobní 487\n",
      "\n",
      "\"Sure\" citations:\n",
      "Total number of citations: 1935\n",
      "Moravský hospodář 4\n",
      "Polední list 57\n",
      "Moravský večerník 118\n",
      "Věstník katolického duchovenstva 220\n",
      "Přítomnost 136\n",
      "Venkov 494\n",
      "Studentský časopis 51\n",
      "Československý zemědělec 48\n",
      "Český učitel 67\n",
      "Čech 567\n",
      "Posel záhrobní 173\n"
     ]
    }
   ],
   "source": [
    "get_overall_result_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations in years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_plot = ['1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939']\n",
    "\n",
    "def ciation_counts_per_year(results_filename:str, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "    \n",
    "    citations = defaultdict(int)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        year = row_dict['date'].split('.')[-1]\n",
    "        citations[year] += 1\n",
    "\n",
    "    return citations\n",
    "\n",
    "\n",
    "def ciation_counts_per_year_not_drop(results_filename:str, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "\n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "    \n",
    "    citations = defaultdict(int)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        year = row_dict['date'].split('.')[-1]\n",
    "        citations[year] += 1\n",
    "\n",
    "    return citations\n",
    "\n",
    "\n",
    "def sure_ciations_counts_per_year(results_filename:str, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "\n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['CITATION'] == True]\n",
    "    \n",
    "    citations = defaultdict(int)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        year = row_dict['date'].split('.')[-1]\n",
    "        citations[year] += 1\n",
    "    \n",
    "    return citations\n",
    "\n",
    "\n",
    "def results_in_years():\n",
    "    print('Filtered citations:')\n",
    "    filtered_citations = ciation_counts_per_year('FILTERED_UNFILTERED_batch_results.csv')\n",
    "    print(filtered_citations)\n",
    "    print('\\nStop subs:')\n",
    "    after_stop_subs = ciation_counts_per_year('ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "    print(after_stop_subs)\n",
    "    print('\\nMultiple attrs:')\n",
    "    multiple_attrs = ciation_counts_per_year_not_drop('MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "    print(multiple_attrs)\n",
    "    print('\\n\"Sure\" citations:')\n",
    "    sure_cites = sure_ciations_counts_per_year('FINAL_MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "    print(sure_cites)\n",
    "\n",
    "    out_df_dict = {'year': [], 'initial results': [], 'filtered stop-subverses': [], 'filtered multiple attributions': [], '\"sure\" citations': []}\n",
    "\n",
    "    for year in years_to_plot:\n",
    "        out_df_dict['year'].append(year)\n",
    "        out_df_dict['initial results'].append(filtered_citations[year])\n",
    "        out_df_dict['filtered stop-subverses'].append(after_stop_subs[year])\n",
    "        out_df_dict['filtered multiple attributions'].append(multiple_attrs[year])\n",
    "        out_df_dict['\"sure\" citations'].append(sure_cites[year])\n",
    "\n",
    "    stats_df = pd.DataFrame(out_df_dict)\n",
    "    stats_df.to_csv(os.path.join(RESULTS_PATH, 'statistics', 'years_results_stats.csv'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered citations:\n",
      "defaultdict(<class 'int'>, {'1927': 3272, '1926': 2684, '1929': 2609, '1928': 2551, '1930': 2899, '1925': 3102, '1934': 2188, '1931': 3126, '1933': 3054, '1932': 2505, '1935': 2023, '1938': 1945, '1937': 2085, '1936': 2067, '1939': 1894})\n",
      "\n",
      "Stop subs:\n",
      "defaultdict(<class 'int'>, {'1927': 1376, '1926': 988, '1928': 881, '1930': 935, '1925': 1422, '1934': 886, '1933': 801, '1932': 738, '1929': 915, '1931': 1198, '1935': 613, '1938': 504, '1936': 526, '1937': 553, '1939': 522})\n",
      "\n",
      "Multiple attrs:\n",
      "defaultdict(<class 'int'>, {'1927': 836, '1926': 659, '1928': 607, '1930': 584, '1925': 882, '1934': 523, '1933': 552, '1932': 480, '1929': 609, '1931': 679, '1935': 374, '1938': 344, '1936': 325, '1937': 342, '1939': 325})\n",
      "\n",
      "\"Sure\" citations:\n",
      "defaultdict(<class 'int'>, {'1934': 161, '1926': 118, '1932': 110, '1925': 179, '1927': 178, '1933': 158, '1929': 139, '1928': 122, '1931': 140, '1930': 123, '1935': 125, '1938': 98, '1937': 92, '1936': 103, '1939': 89})\n"
     ]
    }
   ],
   "source": [
    "results_in_years()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top citations - full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciation_counts_per_verses(results_filename:str, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "    \n",
    "    citations = defaultdict(int)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        verse_id = row_dict['verse_id']\n",
    "        citations[verse_id] += 1\n",
    "\n",
    "    return citations\n",
    "\n",
    "def ciation_counts_per_verses_drop(results_filename:str, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "    \n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "\n",
    "    citations = defaultdict(int)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        verse_id = row_dict['verse_id']\n",
    "        citations[verse_id] += 1\n",
    "\n",
    "    return citations\n",
    "\n",
    "def ciation_counts_per_verses_sure(results_filename:str, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "    \n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['CITATION'] == True]\n",
    "\n",
    "    citations = defaultdict(int)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        verse_id = row_dict['verse_id']\n",
    "        citations[verse_id] += 1\n",
    "\n",
    "    return citations\n",
    "\n",
    "def analyse_verses():\n",
    "    print('Filtered citations:')\n",
    "    filtered_citations = ciation_counts_per_verses('FILTERED_UNFILTERED_batch_results.csv')\n",
    "    print('\\nStop subs:')\n",
    "    after_stop_subs = ciation_counts_per_verses('ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "    print('\\nMultiple attrs:')\n",
    "    multiple_attrs = ciation_counts_per_verses_drop('MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "    print('\\n\"Sure\" citations:')\n",
    "    sure_cites = ciation_counts_per_verses_sure('FINAL_MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "\n",
    "    out_df_dict = {'verse': [], 'initial results': [], 'filtered stop-subverses': [], 'filtered multiple attributions': [], '\"sure\" citations': []}\n",
    "\n",
    "    for verse_id in filtered_citations:\n",
    "        out_df_dict['verse'].append(verse_id)\n",
    "        try:\n",
    "            out_df_dict['initial results'].append(filtered_citations[verse_id])\n",
    "        except:\n",
    "            out_df_dict['initial results'].append(0)\n",
    "        try:            \n",
    "            out_df_dict['filtered stop-subverses'].append(after_stop_subs[verse_id])\n",
    "        except:\n",
    "            out_df_dict['filtered stop-subverses'].append(0)\n",
    "        try:                \n",
    "            out_df_dict['filtered multiple attributions'].append(multiple_attrs[verse_id])\n",
    "        except:\n",
    "            out_df_dict['filtered multiple attributions'].append(0)\n",
    "        try:                \n",
    "            out_df_dict['\"sure\" citations'].append(sure_cites[verse_id])\n",
    "        except:\n",
    "            out_df_dict['\"sure\" citations'].append(0)\n",
    "\n",
    "    stats_df = pd.DataFrame(out_df_dict)\n",
    "    stats_df.to_csv(os.path.join(RESULTS_PATH, 'statistics', 'verse_results_stats.csv'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered citations:\n",
      "\n",
      "Stop subs:\n",
      "\n",
      "Multiple attrs:\n",
      "\n",
      "\"Sure\" citations:\n"
     ]
    }
   ],
   "source": [
    "analyse_verses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciation_counts_per_verses_for_top(results_filename:str, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "    \n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "\n",
    "    citations = defaultdict(int)\n",
    "    verse_texts = defaultdict(list)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        verse_id = row_dict['verse_id']\n",
    "        verse_text = row_dict['verse_text']\n",
    "        citations[verse_id] += 1\n",
    "        if verse_text not in verse_texts[verse_id]:\n",
    "            verse_texts[verse_id].append(verse_text)\n",
    "\n",
    "    return citations, verse_texts\n",
    "\n",
    "\n",
    "def analyse_verses_top():\n",
    "    \"\"\" Here we count only with filtered stop-subs and resolved multiple attributions. \"\"\"\n",
    "    citations, verse_texts = ciation_counts_per_verses_for_top('FINAL_MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "\n",
    "    citation_counts = []\n",
    "    for verse_id in citations:\n",
    "        citation_counts.append(citations[verse_id])\n",
    "\n",
    "    citation_counts.sort(reverse=True)    \n",
    "\n",
    "    out_df_dict = {'verse': [], 'citation count': [], 'verse texts': [], 'filter row': []}\n",
    "\n",
    "    for verse_id in citations:\n",
    "        if citations[verse_id] in citation_counts[:10]:\n",
    "            out_df_dict['verse'].append(verse_id)\n",
    "            out_df_dict['citation count'].append(citations[verse_id])\n",
    "            out_df_dict['verse texts'].append(verse_texts[verse_id])\n",
    "            out_df_dict['filter row'].append('top 10')\n",
    "        elif citations[verse_id] in citation_counts[10:20]:\n",
    "            out_df_dict['verse'].append(verse_id)\n",
    "            out_df_dict['citation count'].append(citations[verse_id])\n",
    "            out_df_dict['verse texts'].append(verse_texts[verse_id])\n",
    "            out_df_dict['filter row'].append('top 20')\n",
    "        elif citations[verse_id] in citation_counts[20:30]:\n",
    "            out_df_dict['verse'].append(verse_id)\n",
    "            out_df_dict['citation count'].append(citations[verse_id])\n",
    "            out_df_dict['verse texts'].append(verse_texts[verse_id])\n",
    "            out_df_dict['filter row'].append('top 30')\n",
    "        else:\n",
    "            out_df_dict['verse'].append(verse_id)\n",
    "            out_df_dict['citation count'].append(citations[verse_id])\n",
    "            out_df_dict['verse texts'].append(verse_texts[verse_id])\n",
    "            out_df_dict['filter row'].append('low')\n",
    "\n",
    "\n",
    "    stats_df = pd.DataFrame(out_df_dict)\n",
    "    stats_df.to_csv(os.path.join(RESULTS_PATH, 'statistics', 'top_verse_results_stats.csv'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_verses_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top citations - to plot in years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_x_verse_ids(x:int, results_filename:str):\n",
    "    \"\"\" This function returns top x citations from results dataframe. \"\"\"\n",
    "\n",
    "    citations, verse_texts = ciation_counts_per_verses_for_top(results_filename)\n",
    "\n",
    "    citation_counts = []\n",
    "    for verse_id in citations:\n",
    "        citation_counts.append(citations[verse_id])\n",
    "\n",
    "    citation_counts.sort(reverse=True)\n",
    "\n",
    "    top_verses = []\n",
    "\n",
    "    for verse_id in citations:\n",
    "        if citations[verse_id] in citation_counts[:x]:\n",
    "            top_verses.append(verse_id)\n",
    "\n",
    "    return top_verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciation_counts_per_verses_in_years(results_filename:str, verese_ids_to_plot:list, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "\n",
    "    citations = defaultdict(dict)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        year = row_dict['date'].split('.')[-1]\n",
    "        verse_id = row_dict['verse_id']\n",
    "        if verse_id in verese_ids_to_plot:\n",
    "            try:\n",
    "                citations[year][verse_id] += 1\n",
    "            except KeyError:\n",
    "                citations[year][verse_id] = 1\n",
    "\n",
    "    return citations\n",
    "\n",
    "\n",
    "def ciation_counts_per_verses_in_years_drop(results_filename:str, verese_ids_to_plot:list, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "    \n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "    \n",
    "    citations = defaultdict(dict)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        year = row_dict['date'].split('.')[-1]\n",
    "        verse_id = row_dict['verse_id']\n",
    "        if verse_id in verese_ids_to_plot:\n",
    "            try:\n",
    "                citations[year][verse_id] += 1\n",
    "            except KeyError:\n",
    "                citations[year][verse_id] = 1\n",
    "\n",
    "    return citations\n",
    "\n",
    "\n",
    "def ciation_counts_per_verses_in_years_sure(results_filename:str, verese_ids_to_plot:list, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "    \n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['CITATION'] == True]\n",
    "\n",
    "    citations = defaultdict(dict)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        year = row_dict['date'].split('.')[-1]\n",
    "        verse_id = row_dict['verse_id']\n",
    "        if verse_id in verese_ids_to_plot:\n",
    "            try:\n",
    "                citations[year][verse_id] += 1\n",
    "            except KeyError:\n",
    "                citations[year][verse_id] = 1\n",
    "\n",
    "\n",
    "def analyse_verse_ids_counts_in_years(top_x:int):\n",
    "    verese_ids_to_plot = return_top_x_verse_ids(x=top_x, results_filename='MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv')\n",
    "\n",
    "    citations = ciation_counts_per_verses_in_years_drop('MA_DUPS_ST_SUBS_FILTERED_UNFILTERED_batch_results.csv', verese_ids_to_plot)\n",
    "\n",
    "    out_df_dict = defaultdict(list)\n",
    "\n",
    "    for year in citations:\n",
    "        out_df_dict['year'].append(year)\n",
    "        for verse_id in verese_ids_to_plot:\n",
    "            try:\n",
    "                out_df_dict[verse_id].append(citations[year][verse_id])\n",
    "            except:\n",
    "                out_df_dict[verse_id].append(0)\n",
    "\n",
    "    stats_df = pd.DataFrame(out_df_dict)\n",
    "    stats_df.to_csv(os.path.join(RESULTS_PATH, 'statistics', 'verses_in_years_results_stats.csv'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_verse_ids_counts_in_years(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Defining subsets books\"\"\"\n",
    "new_testament_books = ['1J', '1K', '1P', '1Te', '1Tm', '2J', '2K', '2P', '2Te', '2Tm', '3J', 'Ef', 'Fm', 'Fp', 'Ga', 'J', 'Jk', 'Ju', 'Ko', 'L', 'Mk', 'Mt', 'R', 'Sk', 'Tit', 'Zd']\n",
    "old_testament_books = ['1Kr', '1Pa', '1S', '2Kr', '2Pa', '2S', 'Abd', 'Abk', 'Ag', 'Am', 'Da', 'Dt', 'Est', 'Ex', 'Ez', 'Ezd', 'Gn', 'Iz', 'Jb', 'Jl', 'Jon', 'Joz', 'Jr', 'Kaz', 'Lv', 'Mal', 'Mi', 'Na', 'Neh', 'Nu', 'Oz', 'Pis', 'Pl', 'Pr', 'Rt', 'Sd', 'Sf', 'Z', 'Za']\n",
    "deuterocanoncal_books = ['1Ma', '2Ma', 'Bar', 'Jud', 'Mou', 'Sir', 'Tob', 'Zuz']\n",
    "gospels = ['Mk', 'Mt', 'L', 'J']\n",
    "\n",
    "\"\"\" Defining subsets verses\"\"\"\n",
    "ten_commandments_verses = ['Ex 20:2', 'Ex 20:3', 'Ex 20:4', 'Ex 20:5', 'Ex 20:6', 'Ex 20:7', 'Ex 20:8', 'Ex 20:9', 'Ex 20:10', 'Ex 20:11', 'Ex 20:12', 'Ex 20:13', 'Ex 20:14', 'Ex 20:15', 'Ex 20:16', 'Ex 20:17', 'Dt 5:6', 'Dt 5:7', 'Dt 5:8', 'Dt 5:9', 'Dt 5:10', 'Dt 5:11', 'Dt 5:12', 'Dt 5:13', 'Dt 5:14', 'Dt 5:15', 'Dt 5:16', 'Dt 5:17', 'Dt 5:18', 'Dt 5:19', 'Dt 5:20', 'Dt 5:21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_name(verse_id:str):\n",
    "    return verse_id.split(' ')[0]\n",
    "\n",
    "def ciation_counts_per_verses_in_years_subset_books(results_filename:str, books_to_plot:list, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "    \n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "\n",
    "    citations = defaultdict(dict)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        year = row_dict['date'].split('.')[-1]\n",
    "        verse_id = row_dict['verse_id']\n",
    "        book_name = get_book_name(verse_id)\n",
    "    \n",
    "        if book_name in books_to_plot:\n",
    "            try:\n",
    "                citations[year][verse_id] += 1\n",
    "            except KeyError:\n",
    "                citations[year][verse_id] = 1\n",
    "\n",
    "    return citations\n",
    "\n",
    "\n",
    "def ciation_counts_per_verses_in_years_subset_verses(results_filename:str, verse_ids_to_plot:list, csv_delimiter=';'):\n",
    "    citations_dataframe = pd.read_csv(os.path.join(RESULTS_PATH, results_filename), quotechar='\"', delimiter=csv_delimiter, encoding='utf-8', index_col=0)\n",
    "    \n",
    "    citations_dataframe = citations_dataframe[citations_dataframe['drop?'] == False]\n",
    "\n",
    "    citations = defaultdict(dict)\n",
    "    \n",
    "    for row_id in citations_dataframe.index:\n",
    "        row_dict = citations_dataframe.loc[row_id].to_dict()\n",
    "        year = row_dict['date'].split('.')[-1]\n",
    "        verse_id = row_dict['verse_id']\n",
    "    \n",
    "        if verse_id in verse_ids_to_plot:\n",
    "            try:\n",
    "                citations[year][verse_id] += 1\n",
    "            except KeyError:\n",
    "                citations[year][verse_id] = 1\n",
    "\n",
    "    return citations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87a9b2b242b38e94a819abbe84d1459ca0c92b18a62bfdb9bd39457d363f0a13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
